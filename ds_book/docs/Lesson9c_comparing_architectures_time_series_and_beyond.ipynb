{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Libraries for fetching Tensorflow and Keras models\n",
    "\n",
    "There are a couple of popular options for fetching CNN, RNN, Transformer, and other kinds of deep learning models for Keras and Tensorflow. Each has it's pros, cons, and varying degrees of maintenance and support:\n",
    "\n",
    "* [Huggingface Models](https://huggingface.co/models) is the largest hub of open source models across all ML frameworks. There are over 1000 Keras models available for download. However, most of the community that uses Huggingface publishes their models in Pytorch (there are over 100,000 Pytorch models). Some of these models are published by the community and are unreviewed, meaning they may be more or less maintained. There are over 100 models released by the Keras team: https://huggingface.co/keras-io.\n",
    "* [Tensorflow Image Models](https://github.com/martinsbruveris/tensorflow-image-models) is a project maintained by an individual open source contributor, Martins Bruveris, a staff applied scientist at Onfido. Available models are listed [here](https://github.com/martinsbruveris/tensorflow-image-models#models), and are sourced from the original paper implementations. The available models come from fundamental developments in deep learning, and include models such as ResNet, a CNN that efficiently learns features using residual connection, allowing for deeper networks, and more recent models like Pyramid Vison Transformer. Each model is profiled for GPU memory requirements and image throughput, a measure of training speed.\n",
    "* [Keras Applications](https://keras.io/api/applications/) is maintained by the Keras Team. It hosts a selection of important models with benchmarks, and unique features such as being able to load models with built in image preprocessing to work with Channel first or Channel last formatted inputs. While it does not support transformer based models, it supports [ConvNeXt](https://arxiv.org/abs/2201.03545) a CNN-based architecture engineered with inspiration from vision transformers that competes favorably with recent vision transformers like [SWIN Transformer](https://arxiv.org/abs/2103.14030).\n",
    "\n",
    "We'll use Huggingface to demo fetching, loading, and comparing model inferences. since Huggingface has the easiest to use and most well supported API for fetching and using deep learning models. Furthermore, many models published by the Keras Team contain corresponding jupyter notebook tutorials demonstrating their use in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvLSTM and alternatives for time series prediction\n",
    "* The curse of dimensionality comes into play even more so when working with deep time series and deep learning.\n",
    "* The \"curse of dimensionality\" is a term often used in machine learning, statistics, and data analysis to refer to the various challenges and problems that arise when dealing with high-dimensional data. As the dimensionality of a dataset increases, the volume of the space increases exponentially, which leads to various issues. \n",
    "* Challenges and aspects related to the curse of dimensionality:\n",
    "  * Sparse Data: As the number of dimensions increases, the amount of data needed to fill the space grows exponentially. Consequently, data becomes sparse. This means that most of the possible combinations of values are not observed, making it harder to identify patterns.\n",
    "  * Distance Measures Become Less Informative: In high dimensions, the distances between data points tend to converge, i.e., all pairs of points seem \"equally far apart\". This is problematic for algorithms like k-nearest neighbors, where distance is a crucial factor.\n",
    "  * Increased Computation: With more dimensions, the computational complexity of many algorithms increases, often exponentially. This means that processing times become longer, and more memory and storage space are needed.\n",
    "  * Risk of Overfitting: With many dimensions, there's an increased likelihood of overfitting the model to the training data. This is because with more features, the model can fit noise or random fluctuations in the training data, leading to poorer generalization to new or unseen data.\n",
    "  * Decreased Model Performance: With an increasing number of irrelevant features, the performance of many machine learning algorithms can degrade.\n",
    "  * Intuition Breaks Down: It's difficult to visualize or comprehend data in very high dimensions, which makes it challenging to understand the structure of the data or the behavior of a model.\n",
    "  * Harder to Ensure Quality: As the number of dimensions increase, ensuring data quality across all dimensions becomes more challenging. Missing or erroneous data can impact results more drastically.\n",
    "* Combating the Curse:\n",
    "  * Feature Selection: This involves identifying and using only the most informative features or dimensions.\n",
    "  * Dimensionality Reduction: Techniques like Principal Component Analysis (PCA), t-distributed Stochastic Neighbor Embedding (t-SNE), and autoencoders can be used to reduce the number of dimensions while retaining most of the data's variability.\n",
    "  * Regularization: Methods like L1 (Lasso) and L2 (Ridge) regularization can be applied to penalize complex models, especially in regression contexts, to prevent overfitting.\n",
    "  * Sampling: Techniques like random sampling or clustering can be used to reduce data size, and thus dimensionality, without losing too much information.\n",
    "  * Ensemble Methods: Combining multiple models (like in random forests) can mitigate some of the risks of high dimensionality.\n",
    "  * Domain Knowledge: Incorporating knowledge from the domain of application can help in intelligently selecting features and understanding the data's underlying structure.\n",
    "* working with time series is difficult in deep learning, the curse of dimensionality comes into play if you try to learn on too many features with too little data\n",
    "* high computational / dataset burden for image timeseries / video prediction\n",
    "* in the remote sensing domain, due to dataset size, best to try smaller models first that convolve over the time dimension isntead of the spatial dimensions\n",
    "* See https://developmentseed.org/blog/2023-06-29-time-travel-pixels and https://arxiv.org/abs/2207.13159 and https://arxiv.org/abs/2304.14065\n",
    "* not all\n",
    "\n",
    "Vision Transformers and CNN alternatives\n",
    "* computational burden is also quadratic with number of self attention layers\n",
    "* ConvMixer is an alternative with lower computational burden, that is CNN and MLP based, available from the Keras team: https://huggingface.co/keras-io/convmixer\n",
    "\n",
    "\n",
    "### Evolution of CNN Architectures for image classification, object detection, and segmentation\n",
    "\n",
    "LeNet (1998):\n",
    "First CNN introduced by LeCun et al. 1998\n",
    "* introduced the convolutional layer: a 3 part process involving convolution, pooling, and nonlinear activation functions\n",
    "* used tanh as the activation function. Later networks replace this with ReLU to address the vanishing gradient problem\n",
    "* unpopular at the time because the lack of software/hardware compatibility for training large networks\n",
    "* SVM still competitive with LeNet on detection at the time\n",
    "\n",
    "AlexNet (2012):\n",
    "* breakout moment for CNNs: error rate 10.8 percentage points lower than next runner up on the ImageNet Large Scale Visual Recognition Challenge\n",
    "* primary result was that deeper networks = higher performance. Later research probes this more, finding that shallow and wide networks also have their place.\n",
    "* computation made feasible with GPUs\n",
    "* used non-saturating ReLU function, better training performance than tanh or sigmoid activation\n",
    "* one of the most widely cited papers 120,000 times on Google Scholar\n",
    "\n",
    "Inception (GoogleNet 2014):\n",
    "* highlights a problem with deep neural networks - large parameter models are more prone to overfitting given fixed dataset size\n",
    "* computational burden of deep vision networks: if two convolutional filters are chained, a uniform increase in their number of filters increases computation quadratically\n",
    "* increases depth and width of the network but with less parameters!\n",
    "* has less parameters than AlexNet, part of a trend towards making smaller, high performing deep neural networks\n",
    "* benefits of less parameters: faster to train, less memory consumption, more efficient learned features\n",
    "* named after the film \"Inception\" - the \"we need to go deeper\" meme\n",
    "* \"For most of the experiments, the models were designed to keep a computational budget of 1.5 billion multiply-adds at inference time, so that the they do not end up to be a purely academic curiosity, but could be put to real world use, even on large datasets, at a reasonable cost.\"\n",
    "* highlights concern that max pooling layers result in a loss of high resolution spatial information\n",
    "* Instead off convolution > max pooling > ReLU, adds a 1x1 convolution to result in convolution ? max pooling > 1x1 convolution > Relu. this is the inception module\n",
    "\n",
    "ResNet (2015):\n",
    "Introduces residuals & skip connections.\n",
    "Enables very deep networks (e.g., 150 layers).\n",
    "Notable Paper: Kaiming He 2016 CVPR “deep residual learning for image recognition”.\n",
    "\n",
    "\n",
    "DenseNet:\n",
    "Enhances residual connections by connecting all layers.\n",
    "\n",
    "\n",
    "U-Net:\n",
    "Architecture for image-to-image tasks, no fully connected layers because it doesn't compress feature information into a single category.\n",
    "Downsamples the image, then upsamples it into a segmentation mask.\n",
    "[Insert: Reference to U-Net Paper]\n",
    "\n",
    "\n",
    "ViT (Vision Transformer):\n",
    "Uses attention to focus on certain parts of images.\n",
    "Image split into patches, flattened, and transformed to embeddings.\n",
    "Pre-training and fine-tuning mechanisms are similar as they are for CNNs.\n",
    "Notable Paper: \"An image is worth 16x16 words: Transformers for image recognition at scale\".\n",
    "\n",
    "\n",
    "### Advanced Concepts: Deep learning architectures for segmentation and change detection in image time series\n",
    "\n",
    "ConvLSTM:\n",
    "Effective for time series data in computer vision.\n",
    "Knowledge Distillation:\n",
    "Teacher-student networks where the student (smaller) network tries to imitate the teacher (larger) network.\n",
    "SCCANet (Spatial and Spectral Channel Attention Network):\n",
    "An evolved form of U-Net with attention mechanisms.\n",
    "Stacked AutoEncoder (SAM?):\n",
    "Focuses on lossless compression.\n",
    "Comparison with PCA and deep autoencoders.\n",
    "Introduction of Variational autoencoders.\n",
    "RaVAEn: Unsupervised learning for change detection.\n",
    "Image Retrieval:\n",
    "Using vector databases for similarity computations.\n",
    "Image Captioning:\n",
    "[Insert: Brief Explanation and Examples]\n",
    "\n",
    "\n",
    "5. Generative Models\n",
    "\n",
    "Generative Adversarial Networks (GANs):\n",
    "Introduction to Generator and Discriminator networks.\n",
    "DCGANs (Deep Convolutional GANs).\n",
    "Applications:\n",
    "Generating human images (\"This person does not exist\").\n",
    "GAN-based style transfer.\n",
    "Age transitions using GANs.\n",
    "Super-resolution.\n",
    "Challenges:\n",
    "Formulating networks to \"compete\" and achieve desired results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatio-temporal transformer STT\n",
    "\n",
    "Spatial transformer\n",
    "\n",
    "Set and t models outperform spatial models\n",
    "\n",
    "Adversarial Label-efficient satellit image change detection\n",
    "- Manual inspection out of reach\n",
    "- Fully automatic solutions reach limitation. Irrelevant changes. Illumination occlusion, noise, blurring\n",
    "- Remove irrelevant changes with normalization, registration, \n",
    "- Reducing the variability\n",
    "- Or model the variability, remove with ML, DNNs SVMs\n",
    "- Hard to know how to model variability\n",
    "- Change detection method with active learning\n",
    "- Q&A model\n",
    "- Finds relevant exemplars to label using adversarial criterion that mixes diversity, representativity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change detection\n",
    "\n",
    "unet+MSOF\n",
    "\n",
    "FC+Siamese\n",
    "\n",
    "CLM: novel model for multi scale features and multi level context\n",
    "\n",
    "EMS-CDNet = uses partition unit for faster feature extraction and merging\n",
    "\n",
    "But it struggled with unstrictly registered datasets\n",
    "\n",
    "M2-CDNET addresses this. Deformable convolution learns offsets between given features to alleviate pixel displacement caused by registration errors. As the scale shrinks…\n",
    "\n",
    "Future work\n",
    "NERF, reduce registration errors and projection bias\n",
    "\n",
    "en.skyearth.org\n",
    "\n",
    "Zhangyj\n",
    "\n",
    "\n",
    "New talk: VHR change detection\n",
    "\n",
    "HCGMNet-CD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges in forecasting and detection for natural disasters\n",
    "\n",
    "Big data\n",
    "Labeling - imbalance, noisy labels\n",
    "Generalization\n",
    "Uncertainty in forecasting\n",
    "Stochastic nature, complex, non-linear interactions between earth system variables\n",
    "Effuse system for fire prediction in Europe assumes pine fuel everywhere\n",
    "\n",
    "Firecube: A daily data cube for the modeling and analysis of wildfires in Greece. Dataset is on zenodo\n",
    "\n",
    "Model types\n",
    "Pixel-wise = RF, XGBOOST\n",
    "Temporal pixel-wise: LSTM\n",
    "Spatiotemporal: ConvLSTM\n",
    "\n",
    "Showed convlstm does best, better than xgboost!\n",
    "\n",
    "Predictive accuracy isn;’t enough, fire brigade wants to know why. Explainable ai\n",
    "Can see what model is paying attention to, which variables and which times\n",
    "\n",
    "Bayesian neural network for uncertainty estimation\n",
    "\n",
    "Instead of scalar weights that are constant, replace them with distributions\n",
    "\n",
    "Used a Unet to segment wildfire forecast\n",
    "\n",
    "Unet doesn’t account for teleconnections. Memory effects from drought from last year\n",
    "\n",
    "TeleViT Teleconnection Vision transformer\n",
    "\n",
    "More resilient to long forecasting windows!\n",
    "\n",
    "62 to 61% AUPRC % instead of 62 to 58% for Unet ++ for long forecasting window\n",
    "\n",
    "Annotated interferograms of volcanos, the segmentation, labels for deformation, activity type\n",
    "\n",
    "ODC and ESDL are full data cube frameworks\n",
    "\n",
    "Datacube challenges, autocorrelation\n",
    "Difficult to model high dim data. East to overfit and need to regularize\n",
    "Computation issues\n",
    "\n",
    "Float 32 are 4 bytes each.\n",
    "\n",
    "Probabilistic ML hands on\n",
    "\n",
    "Discard fraction of most uncertain samples, inference on more certain samples should lead to lower loss. Shows that uncertainty estimate is more accurate if loss decreases with higher certainty sample inference\n",
    "\n",
    "SEASFIRE\n",
    "\n",
    "Uncertainty estimation\n",
    "\n",
    "1. Copernicus foundational models for dim reduction, multi modal data fusion\n",
    "2. Out of distribution generalization\n",
    "3. Earth as a graph,capture teleconnections. Ssls for earth as a graph, long term interactions\n",
    "4. Causality and physics, guided ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Structure of intro to deep learning\n",
    "* Convolution nns\n",
    "    * Convolution + subsampling + () + … + fully connected\n",
    "* Convolved feature creation graphic gif\n",
    "* Activation\n",
    "    * Simplifies learning, makes learning faster\n",
    "    * Avoids saturation issues\n",
    "* Subsampling, example: max pooling. Scale invariance. Params: type filter size, stride\n",
    "* Lacuna et al 1998 LeNet first CNN\n",
    "* Inception (GoogleNet 2014) convolutions with 3x3 and 5x5\n",
    "* Residuals = bypass some convolutions and add skip connections. ResNet\n",
    "* Allows very deep networks. 150 layers\n",
    "* Kaiming He 2016 CVPR “deep residual learning for image recognition”\n",
    "* DenseNet. Pushes residual connections by connecting everything to everything\n",
    "* Unet - arch that goes from images to images (paper?)\n",
    "* Shrinks image down that samples it back up\n",
    "* ViT\n",
    "    * Attention makes system focus on specific instances\n",
    "    * Split image into fixed patches, sizes\n",
    "    * Flatten image patches\n",
    "    * Create features, embeddings for patches\n",
    "    * Positional embeddings\n",
    "    * Feed embedding sequence to transformer encoder\n",
    "    * Pretrain it with image labels\n",
    "    * Retrain many samples, fine-tune\n",
    "    * An image is worth 16x16 words: Transformers for image recognition at scale\n",
    "* ConvLSTM for time series\n",
    "* Knowledge distillation, teacher student networks. smaller student network tries to approximate teacher\n",
    "* Show different date types, classification to instance segmentation\n",
    "* Segnet = cnn encoder decoder. Cone + batch norm + relu. Pooling. Upsampling. Softmax\n",
    "* Segnet different from net slightly. No bottleneck for segment. Segment is fully connected. 2017\n",
    "* Unet more popular than segment\n",
    "* 3D Unet possible. 4D Unet possible\n",
    "\n",
    "* SCCANet Spatial and Spectral Channel Attention Network\n",
    "* The size of intermediate layer sis very trial and error and empirical if it works\n",
    "* Basically still a Unet\n",
    "* There’s a lot of work that is basically just Unet or X Architecture that is published\n",
    "* Focus on using established architectures published from reliable sources in easy to use libraries/ml frameworks\n",
    "\n",
    "Stacked AutoEncoder (SAM?)\n",
    "* Lossless compression\n",
    "* Comparison to PCA, retrieving input in the output\n",
    "* Even with 730 or 30 intermediate dimensions\n",
    "* Deep auto encoders\n",
    "* Intermediate representations are decoded\n",
    "* Network extracts higher level features \n",
    "* Variational auto encoders\n",
    "    * Encoding distribution is regularized during training to ensure latent space can generate new data\n",
    "    * Difference between auto encoder to variational auto encoder\n",
    "* RaVAEn unsupervised learning for change detection\n",
    "    * Detecting floods without labels\n",
    "    * Show many examples of unlabeled flooding\n",
    "    * Time series of images that include after event image\n",
    "    * Is it necessary to order the after event image as the final image in the time series?\n",
    "    * Need geographic representation\n",
    "    * Used Sentinel-2. 4 before event 1 after event\n",
    "    * Cosine baseline vs cosine embeddings?\n",
    "    * Cosine embedding matched the true label more\n",
    "* Image retrieval - vector database\n",
    "    * Query image to do similarity computation\n",
    "* Image captioning\n",
    "* Generators?\n",
    "    * GANS start from random inputs and generate images\n",
    "    * Adversarial part is that two networks compete. One generates images, another network generates predictions if results from counterpart are fake or not\n",
    "    * Generator network and discriminator network\n",
    "    * DCGANs, deep convolutional generative adversarial networks\n",
    "    * This person does not exist, old news, better models now\n",
    "    * Gan based style transfer\n",
    "    * Gan transitions (aging)\n",
    "    * No ground truth!\n",
    "    * Gans for super resolution. Nothing new in terms of arch. CNNS, max pooling, RELU, sigmoid, etc.\n",
    "    * Problem statement with GANs is formulating networks to connect and battle each other to arrive at a specific objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeoFM SAM notes\n",
    "\n",
    "Presto\n",
    "* Operates on pixel time series of multi sensors, multi channel\n",
    "* Channel group embedding, positional, embedding for lagoon, and month embedding\n",
    "* Somewhat marginal but clear improvement over other approach (also time series approach)\n",
    "* Geographic representation across hemispheres and ecoregion\n",
    "* Used dynamic world as an input and to stratify\n",
    "* Presto can be used as feature extractor (for random forest and regression) or fine-tuning the encoder and linear transformation\n",
    "* Main comparison is to Task Informed Meta Learning\n",
    "* Presto is fully self supervised, computationally more efficient than image based approaches\n",
    "* Also beats SatMAE\n",
    "* Also tested fully supervised presto to measure effect of arch vs the self supervision training regime\n",
    "* Didn’t really understand all presto tables, especially table 6\n",
    "* Self supervised presto beats fully supervised presto for particular tasks\n",
    "\n",
    "\n",
    "MedSAM\n",
    "* Strong qualitative improvement in fuzzy boundary objects\n",
    "* Text encoding with CLIP, todo read\n",
    "* SAM can generate object, part, and subpart masks, 4 each. Todo enable?\n",
    "* The mode matters a lot. What’s best for wall to wall mapping…. Segment anything mode has no semantic labels\n",
    "* What’s best for fuzzy objects? Probably box? Sometimes multi point? When does part and subpart matter?\n",
    "* Box == less trial and error\n",
    "* They freeze the encoder\n",
    "* Only mask decoder fine tuned\n",
    "* Pregenerated all training image embeddings\n",
    "* Image encoder resizes images by default to 3x1024x1024, yuck?\n",
    "Through the fine-tuning of SAM on medical image datasets, MedSAM has greatly enhanced the model’s ability to identify challenging segmentation targets. Specifically, MedSAM has demonstrated three significant improvements over the pre-trained SAM. Firstly, MedSAM has improved the model’s ability to identify small objects, even when multiple segmentation targets are present within the bounding box prompt. Secondly, the model has shown more robustness towards weak boundaries in various modalities, such as lesion and left ventricle segmen- tation in ultrasound and brain MR images, respectively. Finally, MedSAM has effectively reduced interference from high-contrast objects surrounding the seg- mentation target, resulting in fewer outliers. \n",
    "* Dice similarity saw huge improvement for nearly all categories, table 1 and 2\n",
    "* Scribble based prompt, interesting idea\n",
    "https://github.com/MIC-DKFZ/napari-sam \n",
    "\n",
    "Sam-adapter-med\n",
    "* Different group almost same time\n",
    "* Different “fine-tuning” method with prompting. Throws away sAM decoder and encoder is frozen\n",
    "* Uses two MLP layers to supply prompts to the encoder\n",
    "* They seem to get great results, unclear if they are not using any prompts at all? Seems like these predictions are promptless.\n",
    "* So better results on specific tasks with adapter fine tuning for each task. Less generalizability for each adapted model and you lose promptability?\n",
    "\n",
    "Personalize-SAM\n",
    "- Lightning fast finetuning variant PerSAM-F\n",
    "- Training free variant PerSAM using one example image and mask\n",
    "- Personalizing sam to segment unique visual concepts (your pet dog)\n",
    "- Learns the best mask scale for a particular problem, handling the object/part/subpart selection choice that SAM offers\n",
    "- This seems unique from the adapter and traditional fine tuning approach\n",
    "- “achieves leading performance on our annotated PerSeg dataset” lol\n",
    "- Mainly compares to seggpt no their own dataset, not other segmentation approaches like sam adapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
