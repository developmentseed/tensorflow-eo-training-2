{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* Understand RNNs and the differences in structure from CNNs\n",
    "* Cover a short history of RNNs for time series forecasting and prediction\n",
    "* Cover challenges with trianing RNNs and deploying\n",
    "* Cover CNN and Transformer models for time series prediction and how they address RNN challenges. were state of the art for text and time seriesb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution of RNN Architectures for forecasting and change detection in 1-D and imagery sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now we have discussed dealing with fixed-length datasets consisting of 3 band images. CNNs work great with these fixed length datasets, but when we need to understand the relationship between images in a sequence, we need to adopt a more complex structure that models the interaction along the time deimsnion. \n",
    "\n",
    "Recurrent Neural Networks, or RNNs, address modeling variable length sequences. These can be text, 1 dimensional time series, or sequences of images, including video. Unlike CNNs, RNNs have a concept of \"hidden state\" for each time step, which is the output of the computation from all of the hidden layer sin the RNN. This hidden state is passed as an input to the next time step in an RNN inaddition to the sequence input. Below, is an example using an RNN for next letter prediction to model the letter \"s\" in the word \"dogs\".\n",
    "\n",
    ":::{figure-md} LeNetFig\n",
    "<img src=\"https://d2l.ai/_images/lenet.svg\" width=\"450px\">\n",
    "\n",
    "[Ben Khuong, The Basics of Recurrent Neural Networks (RNNs)](https://d2l.ai/_images/lenet.svg) from \"Dive into Deep Learning\" by d2l.ai, used under CC BY-SA 4.0\n",
    ":::\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvLSTM and alternatives for time series prediction\n",
    "* The \"curse of dimensionality\" comes into play even more so when working with deep time series and deep learning.\n",
    "* The \"curse of dimensionality\" is a term often used in machine learning, statistics, and data analysis to refer to the various challenges and problems that arise when dealing with high-dimensional data. As the dimensionality of a dataset increases, the volume of the space increases exponentially, which leads to various issues. \n",
    "* Challenges and aspects related to the curse of dimensionality:\n",
    "  * Sparse Data: As the number of dimensions increases, the amount of data needed to fill the space grows exponentially. Consequently, data becomes sparse. This means that most of the possible combinations of values are not observed, making it harder to identify patterns.\n",
    "  * Distance Measures Become Less Informative: In high dimensions, the distances between data points tend to converge, i.e., all pairs of points seem \"equally far apart\". This is problematic for algorithms like k-nearest neighbors, where distance is a crucial factor.\n",
    "  * Increased Computation: With more dimensions, the computational complexity of many algorithms increases, often exponentially. This means that processing times become longer, and more memory and storage space are needed.\n",
    "  * Risk of Overfitting: With many dimensions, there's an increased likelihood of overfitting the model to the training data. This is because with more features, the model can fit noise or random fluctuations in the training data, leading to poorer generalization to new or unseen data.\n",
    "  * Decreased Model Performance: With an increasing number of irrelevant features, the performance of many machine learning algorithms can degrade.\n",
    "  * Intuition Breaks Down: It's difficult to visualize or comprehend data in very high dimensions, which makes it challenging to understand the structure of the data or the behavior of a model.\n",
    "  * Harder to Ensure Quality: As the number of dimensions increase, ensuring data quality across all dimensions becomes more challenging. Missing or erroneous data can impact results more drastically.\n",
    "* Combating the Curse:\n",
    "  * Feature Selection: This involves identifying and using only the most informative features or dimensions.\n",
    "  * Dimensionality Reduction: Techniques like Principal Component Analysis (PCA), t-distributed Stochastic Neighbor Embedding (t-SNE), and autoencoders can be used to reduce the number of dimensions while retaining most of the data's variability.\n",
    "  * Regularization: Methods like L1 (Lasso) and L2 (Ridge) regularization can be applied to penalize complex models, especially in regression contexts, to prevent overfitting.\n",
    "  * Sampling: Techniques like random sampling or clustering can be used to reduce data size, and thus dimensionality, without losing too much information.\n",
    "  * Ensemble Methods: Combining multiple models (like in random forests) can mitigate some of the risks of high dimensionality.\n",
    "  * Domain Knowledge: Incorporating knowledge from the domain of application can help in intelligently selecting features and understanding the data's underlying structure.\n",
    "* working with time series is difficult in deep learning, the curse of dimensionality comes into play if you try to learn on too many features with too little data\n",
    "* high computational / dataset burden for image timeseries / video prediction\n",
    "* in the remote sensing domain, due to dataset size, best to try smaller models first that convolve over the time dimension instead of the spatial dimensions\n",
    "* See https://developmentseed.org/blog/2023-06-29-time-travel-pixels and https://arxiv.org/abs/2207.13159 and https://arxiv.org/abs/2304.14065\n",
    "\n",
    "Vision Transformers and CNN alternatives\n",
    "* computational burden is also quadratic with number of self attention layers\n",
    "* ConvMixer is an alternative with lower computational burden, that is CNN and MLP based, available from the Keras team: https://huggingface.co/keras-io/convmixer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Architecture for image-to-image tasks, no fully connected layers because it doesn't compress feature information into a single category.\n",
    "Downsamples the image, then upsamples it into a segmentation mask.\n",
    "[Insert: Reference to U-Net Paper]\n",
    "\n",
    "\n",
    "ViT (Vision Transformer):\n",
    "Uses attention to focus on certain parts of images.\n",
    "Image split into patches, flattened, and transformed to embeddings.\n",
    "Pre-training and fine-tuning mechanisms are similar as they are for CNNs.\n",
    "Notable Paper: \"An image is worth 16x16 words: Transformers for image recognition at scale\".\n",
    "\n",
    "\n",
    "### Advanced Concepts: Deep learning architectures for segmentation and change detection in image time series\n",
    "\n",
    "ConvLSTM:\n",
    "Effective for time series data in computer vision.\n",
    "Knowledge Distillation:\n",
    "Teacher-student networks where the student (smaller) network tries to imitate the teacher (larger) network.\n",
    "SCCANet (Spatial and Spectral Channel Attention Network):\n",
    "An evolved form of U-Net with attention mechanisms.\n",
    "Stacked AutoEncoder (SAM?):\n",
    "Focuses on lossless compression.\n",
    "Comparison with PCA and deep autoencoders.\n",
    "Introduction of Variational autoencoders.\n",
    "RaVAEn: Unsupervised learning for change detection.\n",
    "Image Retrieval:\n",
    "Using vector databases for similarity computations.\n",
    "Image Captioning:\n",
    "[Insert: Brief Explanation and Examples]\n",
    "\n",
    "\n",
    "5. Generative Models\n",
    "\n",
    "Generative Adversarial Networks (GANs):\n",
    "Introduction to Generator and Discriminator networks.\n",
    "DCGANs (Deep Convolutional GANs).\n",
    "Applications:\n",
    "Generating human images (\"This person does not exist\").\n",
    "GAN-based style transfer.\n",
    "Age transitions using GANs.\n",
    "Super-resolution.\n",
    "Challenges:\n",
    "Formulating networks to \"compete\" and achieve desired results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatio-temporal transformer STT\n",
    "\n",
    "Spatial transformer\n",
    "\n",
    "Set and t models outperform spatial models\n",
    "\n",
    "Adversarial Label-efficient satellit image change detection\n",
    "- Manual inspection out of reach\n",
    "- Fully automatic solutions reach limitation. Irrelevant changes. Illumination occlusion, noise, blurring\n",
    "- Remove irrelevant changes with normalization, registration, \n",
    "- Reducing the variability\n",
    "- Or model the variability, remove with ML, DNNs SVMs\n",
    "- Hard to know how to model variability\n",
    "- Change detection method with active learning\n",
    "- Q&A model\n",
    "- Finds relevant exemplars to label using adversarial criterion that mixes diversity, representativity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change detection\n",
    "\n",
    "unet+MSOF\n",
    "\n",
    "FC+Siamese\n",
    "\n",
    "CLM: novel model for multi scale features and multi level context\n",
    "\n",
    "EMS-CDNet = uses partition unit for faster feature extraction and merging\n",
    "\n",
    "But it struggled with unstrictly registered datasets\n",
    "\n",
    "M2-CDNET addresses this. Deformable convolution learns offsets between given features to alleviate pixel displacement caused by registration errors. As the scale shrinks…\n",
    "\n",
    "Future work\n",
    "NERF, reduce registration errors and projection bias\n",
    "\n",
    "en.skyearth.org\n",
    "\n",
    "Zhangyj\n",
    "\n",
    "\n",
    "New talk: VHR change detection\n",
    "\n",
    "HCGMNet-CD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges in forecasting and detection for natural disasters\n",
    "\n",
    "Big data\n",
    "Labeling - imbalance, noisy labels\n",
    "Generalization\n",
    "Uncertainty in forecasting\n",
    "Stochastic nature, complex, non-linear interactions between earth system variables\n",
    "Effuse system for fire prediction in Europe assumes pine fuel everywhere\n",
    "\n",
    "Firecube: A daily data cube for the modeling and analysis of wildfires in Greece. Dataset is on zenodo\n",
    "\n",
    "Model types\n",
    "Pixel-wise = RF, XGBOOST\n",
    "Temporal pixel-wise: LSTM\n",
    "Spatiotemporal: ConvLSTM\n",
    "\n",
    "Showed convlstm does best, better than xgboost!\n",
    "\n",
    "Predictive accuracy isn;’t enough, fire brigade wants to know why. Explainable ai\n",
    "Can see what model is paying attention to, which variables and which times\n",
    "\n",
    "Bayesian neural network for uncertainty estimation\n",
    "\n",
    "Instead of scalar weights that are constant, replace them with distributions\n",
    "\n",
    "Used a Unet to segment wildfire forecast\n",
    "\n",
    "Unet doesn’t account for teleconnections. Memory effects from drought from last year\n",
    "\n",
    "TeleViT Teleconnection Vision transformer\n",
    "\n",
    "More resilient to long forecasting windows!\n",
    "\n",
    "62 to 61% AUPRC % instead of 62 to 58% for Unet ++ for long forecasting window\n",
    "\n",
    "Annotated interferograms of volcanos, the segmentation, labels for deformation, activity type\n",
    "\n",
    "ODC and ESDL are full data cube frameworks\n",
    "\n",
    "Datacube challenges, autocorrelation\n",
    "Difficult to model high dim data. East to overfit and need to regularize\n",
    "Computation issues\n",
    "\n",
    "Float 32 are 4 bytes each.\n",
    "\n",
    "Probabilistic ML hands on\n",
    "\n",
    "Discard fraction of most uncertain samples, inference on more certain samples should lead to lower loss. Shows that uncertainty estimate is more accurate if loss decreases with higher certainty sample inference\n",
    "\n",
    "SEASFIRE\n",
    "\n",
    "Uncertainty estimation\n",
    "\n",
    "1. Copernicus foundational models for dim reduction, multi modal data fusion\n",
    "2. Out of distribution generalization\n",
    "3. Earth as a graph,capture teleconnections. Ssls for earth as a graph, long term interactions\n",
    "4. Causality and physics, guided ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Structure of intro to deep learning\n",
    "* Convolution nns\n",
    "    * Convolution + subsampling + () + … + fully connected\n",
    "* Convolved feature creation graphic gif\n",
    "* Activation\n",
    "    * Simplifies learning, makes learning faster\n",
    "    * Avoids saturation issues\n",
    "* Subsampling, example: max pooling. Scale invariance. Params: type filter size, stride\n",
    "* Lacuna et al 1998 LeNet first CNN\n",
    "* Inception (GoogleNet 2014) convolutions with 3x3 and 5x5\n",
    "* Residuals = bypass some convolutions and add skip connections. ResNet\n",
    "* Allows very deep networks. 150 layers\n",
    "* Kaiming He 2016 CVPR “deep residual learning for image recognition”\n",
    "* DenseNet. Pushes residual connections by connecting everything to everything\n",
    "* Unet - arch that goes from images to images (paper?)\n",
    "* Shrinks image down that samples it back up\n",
    "* ViT\n",
    "    * Attention makes system focus on specific instances\n",
    "    * Split image into fixed patches, sizes\n",
    "    * Flatten image patches\n",
    "    * Create features, embeddings for patches\n",
    "    * Positional embeddings\n",
    "    * Feed embedding sequence to transformer encoder\n",
    "    * Pretrain it with image labels\n",
    "    * Retrain many samples, fine-tune\n",
    "    * An image is worth 16x16 words: Transformers for image recognition at scale\n",
    "* ConvLSTM for time series\n",
    "* Knowledge distillation, teacher student networks. smaller student network tries to approximate teacher\n",
    "* Show different date types, classification to instance segmentation\n",
    "* Segnet = cnn encoder decoder. Cone + batch norm + relu. Pooling. Upsampling. Softmax\n",
    "* Segnet different from net slightly. No bottleneck for segment. Segment is fully connected. 2017\n",
    "* Unet more popular than segment\n",
    "* 3D Unet possible. 4D Unet possible\n",
    "\n",
    "* SCCANet Spatial and Spectral Channel Attention Network\n",
    "* The size of intermediate layer sis very trial and error and empirical if it works\n",
    "* Basically still a Unet\n",
    "* There’s a lot of work that is basically just Unet or X Architecture that is published\n",
    "* Focus on using established architectures published from reliable sources in easy to use libraries/ml frameworks\n",
    "\n",
    "Stacked AutoEncoder (SAM?)\n",
    "* Lossless compression\n",
    "* Comparison to PCA, retrieving input in the output\n",
    "* Even with 730 or 30 intermediate dimensions\n",
    "* Deep auto encoders\n",
    "* Intermediate representations are decoded\n",
    "* Network extracts higher level features \n",
    "* Variational auto encoders\n",
    "    * Encoding distribution is regularized during training to ensure latent space can generate new data\n",
    "    * Difference between auto encoder to variational auto encoder\n",
    "* RaVAEn unsupervised learning for change detection\n",
    "    * Detecting floods without labels\n",
    "    * Show many examples of unlabeled flooding\n",
    "    * Time series of images that include after event image\n",
    "    * Is it necessary to order the after event image as the final image in the time series?\n",
    "    * Need geographic representation\n",
    "    * Used Sentinel-2. 4 before event 1 after event\n",
    "    * Cosine baseline vs cosine embeddings?\n",
    "    * Cosine embedding matched the true label more\n",
    "* Image retrieval - vector database\n",
    "    * Query image to do similarity computation\n",
    "* Image captioning\n",
    "* Generators?\n",
    "    * GANS start from random inputs and generate images\n",
    "    * Adversarial part is that two networks compete. One generates images, another network generates predictions if results from counterpart are fake or not\n",
    "    * Generator network and discriminator network\n",
    "    * DCGANs, deep convolutional generative adversarial networks\n",
    "    * This person does not exist, old news, better models now\n",
    "    * Gan based style transfer\n",
    "    * Gan transitions (aging)\n",
    "    * No ground truth!\n",
    "    * Gans for super resolution. Nothing new in terms of arch. CNNS, max pooling, RELU, sigmoid, etc.\n",
    "    * Problem statement with GANs is formulating networks to connect and battle each other to arrive at a specific objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeoFM SAM notes\n",
    "\n",
    "Presto\n",
    "* Operates on pixel time series of multi sensors, multi channel\n",
    "* Channel group embedding, positional, embedding for lagoon, and month embedding\n",
    "* Somewhat marginal but clear improvement over other approach (also time series approach)\n",
    "* Geographic representation across hemispheres and ecoregion\n",
    "* Used dynamic world as an input and to stratify\n",
    "* Presto can be used as feature extractor (for random forest and regression) or fine-tuning the encoder and linear transformation\n",
    "* Main comparison is to Task Informed Meta Learning\n",
    "* Presto is fully self supervised, computationally more efficient than image based approaches\n",
    "* Also beats SatMAE\n",
    "* Also tested fully supervised presto to measure effect of arch vs the self supervision training regime\n",
    "* Didn’t really understand all presto tables, especially table 6\n",
    "* Self supervised presto beats fully supervised presto for particular tasks\n",
    "\n",
    "\n",
    "MedSAM\n",
    "* Strong qualitative improvement in fuzzy boundary objects\n",
    "* Text encoding with CLIP, todo read\n",
    "* SAM can generate object, part, and subpart masks, 4 each. Todo enable?\n",
    "* The mode matters a lot. What’s best for wall to wall mapping…. Segment anything mode has no semantic labels\n",
    "* What’s best for fuzzy objects? Probably box? Sometimes multi point? When does part and subpart matter?\n",
    "* Box == less trial and error\n",
    "* They freeze the encoder\n",
    "* Only mask decoder fine tuned\n",
    "* Pregenerated all training image embeddings\n",
    "* Image encoder resizes images by default to 3x1024x1024, yuck?\n",
    "Through the fine-tuning of SAM on medical image datasets, MedSAM has greatly enhanced the model’s ability to identify challenging segmentation targets. Specifically, MedSAM has demonstrated three significant improvements over the pre-trained SAM. Firstly, MedSAM has improved the model’s ability to identify small objects, even when multiple segmentation targets are present within the bounding box prompt. Secondly, the model has shown more robustness towards weak boundaries in various modalities, such as lesion and left ventricle segmen- tation in ultrasound and brain MR images, respectively. Finally, MedSAM has effectively reduced interference from high-contrast objects surrounding the seg- mentation target, resulting in fewer outliers. \n",
    "* Dice similarity saw huge improvement for nearly all categories, table 1 and 2\n",
    "* Scribble based prompt, interesting idea\n",
    "https://github.com/MIC-DKFZ/napari-sam \n",
    "\n",
    "Sam-adapter-med\n",
    "* Different group almost same time\n",
    "* Different “fine-tuning” method with prompting. Throws away sAM decoder and encoder is frozen\n",
    "* Uses two MLP layers to supply prompts to the encoder\n",
    "* They seem to get great results, unclear if they are not using any prompts at all? Seems like these predictions are promptless.\n",
    "* So better results on specific tasks with adapter fine tuning for each task. Less generalizability for each adapted model and you lose promptability?\n",
    "\n",
    "Personalize-SAM\n",
    "- Lightning fast finetuning variant PerSAM-F\n",
    "- Training free variant PerSAM using one example image and mask\n",
    "- Personalizing sam to segment unique visual concepts (your pet dog)\n",
    "- Learns the best mask scale for a particular problem, handling the object/part/subpart selection choice that SAM offers\n",
    "- This seems unique from the adapter and traditional fine tuning approach\n",
    "- “achieves leading performance on our annotated PerSeg dataset” lol\n",
    "- Mainly compares to seggpt no their own dataset, not other segmentation approaches like sam adapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ds_book]",
   "language": "python",
   "name": "conda-env-ds_book-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
