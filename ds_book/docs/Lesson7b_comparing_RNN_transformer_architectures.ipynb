{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* Understand RNNs and the differences in structure from CNNs\n",
    "* Cover a short history of RNNs for time series forecasting and prediction\n",
    "* Cover challenges with trianing RNNs and deploying\n",
    "* Cover CNN and Transformer models for time series prediction and how they address RNN challenges. were state of the art for text and time seriesb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Architectures for forecasting and change detection in 1-D and imagery sequences\n",
    "\n",
    "Up until now we have discussed dealing with fixed-length datasets consisting of 3 band images. CNNs work great with these fixed length datasets, but when we need to understand the relationship between images in a sequence, we need to adopt a more complex structure that models the interaction along the time deimsnion. \n",
    "\n",
    "Recurrent Neural Networks, or RNNs, address modeling variable length sequences. These can be text, 1 dimensional time series, or sequences of images, including video. Unlike CNNs, RNNs have a concept of \"hidden state\" for each time step, which is the output of the computation from all of the hidden layer sin the RNN. This hidden state is passed as an input to the next time step in an RNN inaddition to the sequence input. Below, is an example using an RNN for next letter prediction to model the letter \"s\" in the word \"dogs\".\n",
    "\n",
    ":::{figure-md} RNNFig\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*dyfwJJuGT2Svy10iYAVDEQ.png\" width=\"450px\">\n",
    "\n",
    "[Ben Khuong, The Basics of Recurrent Neural Networks (RNNs)](https://miro.medium.com/v2/resize:fit:720/format:webp/1*dyfwJJuGT2Svy10iYAVDEQ.png)\n",
    ":::\n",
    "\n",
    "Here we see that the hidden states, \"h\", must be computed from individual samples from the sequence. The nodes of the hidden layers, \"a\", represent the learned weights and biases for a particular hidden state. Because we need to compute the hidden states for each sample given the preceding hidden state, we cannot compute hidden states in parallel, and can only parallelize the computation of one hidden state at a time. In other words, each state must be computed asynchronously.\n",
    "\n",
    "This has susbtantial implications for the computational burden of RNNs, it is difficult to parallelize RNN computation, making them take longer to train for non-recurrent architectures. We'll revisit this limitations when discussing alternatives to RNNs.\n",
    "\n",
    "The recurrent connections above relate each time step to it's adjacent time steps. This is akin to the network having a very short term meory biases toward length t=1. While previous inputs farther than t=1 can have an influence on a particular hidden state, regular RNNs suffer from the vanishing (or exploding) gradient problem in that the partial derivatives (gradients) for parameters that are very far away can become very small, eventually reaching 0, or very large. However, when dealing with land cover modeling in remote sensing time series, we often need to model processes that occur over many different time scales, such as the influence of seasonality, a preceding year's snowfall or precipitation effects on the current year's vegetation, and the influence of yesterday's commercial logging activity on today's forest cover. These complex, multi-scale interactions require architectures that can model both short term and longer term connections between samples without numerical instability. Long Term, Short Term memory networks, or LSTMs, were developed as an answer to these more complex sequence modeling problems and to address the issue of vanishing gradients. While LSTMs do not handle the issue of computational training burden, let's review them so we can compare how other architectures address the challenge of computational burden.\n",
    "\n",
    "### Long Term Short Term Memory Networks (Hochreiter and Schmidhuber, 1997)\n",
    "\n",
    "LSTM's introduce the concept of \"state\" to the RNN computation, which we'll refer to as the \"memory block\". The memory block allows an LSTM to keep track of what learned context to forget, what to add or update, and how much of it to pass on to the next time step. These three pieces of information flows are controlled by *gates*. These gates reduce the vanishing gradient problem by allowing inputs to flow through the network without computing uninformative gradients, and by reducing the distance of connections between pieces of a sequence that are far away. These gates are applied multiplicitavely and are computed with sigmoid activation functions. A helpful analogy is that the memory block state is the RAM of your computer, keeping updated information readily accessible to the LSTM.\n",
    "\n",
    ":::{figure-md} LSTMRNNFig\n",
    "<img src=\"https://d2l.ai/_images/lstm-2.svg\" width=\"450px\">\n",
    "\n",
    "[Long Short-Term Memory (LSTM)](https://d2l.ai/_images/lstm-2.svg) from \"Dive into Deep Learning\" by d2l.ai, used under CC BY-SA 4.0\n",
    ":::\n",
    "\n",
    "In the above figure we see:\n",
    "\n",
    "1. The input gate controls how much of the input should be used to influence the internal state of the current memory cell.\n",
    "1. The forget gate controls if the input hidden state should be forgotten so it does not influence subsequent hidden states.\n",
    "1. The output gate deterimens if the output should be influenced by the current memory cell. \n",
    "\n",
    "LSTMs and related RNN architectures such as bi-directional RNNs (Schuster and Paliwal, 1997) were state of the art for sequence prediction from 2011 until 2017, when Transformers were introduced. They're still relevant today in that aspects of recurrent connections are influential in transformer and CNN-based architectures. However, the core limitation of RNNs, that they cannot be traine din parallel, makes them less desirable of an option for training larger models on large image datasets. Let's look at how Transformer and other simpler architectures address this problem and show state of the art performance on sequence prediction in general and image sequence prediction.\n",
    "\n",
    "\n",
    "### Encoder Decoder Architectures and The Transformer (Vaswani et al. 2017)\n",
    "\n",
    "For nearly 30 years, CNNs have dominated computer vision and LSTMs have dominated natural language processing. While mahy advancements have been made that we have discusse din prior lessons, including new activation funct6ions (ReLU), training improvements like Batch Norm, and architectural improvements like residual connections, the same classical architectures (CNNs, and RNNs) have maintined their presence in the state of the art. The deep learning landscape has experienced a somewhat recent sea change in fundamantel computing architecture with the introduction of the Transforme and self-attention.\n",
    "\n",
    "Originally in Vaswani et al. 2017 the transformer was introduced for transuction tasks: converting input natural language sequences to output natural language sequences, i.e. next word prediction. After some time, in 2020, Dosovitsky et al. 2020 showed that the transformer architecture could achieve near state of the art results on image classification compared to more complex CNN-based architectures, with lower computational cost to train.\n",
    "\n",
    "\n",
    "\n",
    "When dealing with image time series, we typically either want to\n",
    "\n",
    "1. do pixel-wise segmentation of the time series at each time step, producing an equivalent length time series of maps\n",
    "1. predict a change map for the time series, which can will be of a different variable length depending on how we measure change.\n",
    "\n",
    "In either case, we are converting an image sequence to another image sequence. Even in the case of single date imagery, we are converting a sequence of bands to a a sequence of length one. Sequence to sequence models can be addressed by Encoder-Decoder architectures, where an Encoder computes image features and a decoder uses those image features to make a prediction. Encoder-Decoder models are powerful because each section can be trained and used for inference independently.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[i] [Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need.](https://arxiv.org/abs/1706.03762)\n",
    "[ii] [Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Uszkoreit, J. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.](https://arxiv.org/abs/2010.11929)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvLSTM and alternatives for time series prediction\n",
    "* The \"curse of dimensionality\" comes into play even more so when working with deep time series and deep learning.\n",
    "* The \"curse of dimensionality\" is a term often used in machine learning, statistics, and data analysis to refer to the various challenges and problems that arise when dealing with high-dimensional data. As the dimensionality of a dataset increases, the volume of the space increases exponentially, which leads to various issues. \n",
    "* Challenges and aspects related to the curse of dimensionality:\n",
    "  * Sparse Data: As the number of dimensions increases, the amount of data needed to fill the space grows exponentially. Consequently, data becomes sparse. This means that most of the possible combinations of values are not observed, making it harder to identify patterns.\n",
    "  * Distance Measures Become Less Informative: In high dimensions, the distances between data points tend to converge, i.e., all pairs of points seem \"equally far apart\". This is problematic for algorithms like k-nearest neighbors, where distance is a crucial factor.\n",
    "  * Increased Computation: With more dimensions, the computational complexity of many algorithms increases, often exponentially. This means that processing times become longer, and more memory and storage space are needed.\n",
    "  * Risk of Overfitting: With many dimensions, there's an increased likelihood of overfitting the model to the training data. This is because with more features, the model can fit noise or random fluctuations in the training data, leading to poorer generalization to new or unseen data.\n",
    "  * Decreased Model Performance: With an increasing number of irrelevant features, the performance of many machine learning algorithms can degrade.\n",
    "  * Intuition Breaks Down: It's difficult to visualize or comprehend data in very high dimensions, which makes it challenging to understand the structure of the data or the behavior of a model.\n",
    "  * Harder to Ensure Quality: As the number of dimensions increase, ensuring data quality across all dimensions becomes more challenging. Missing or erroneous data can impact results more drastically.\n",
    "* Combating the Curse:\n",
    "  * Feature Selection: This involves identifying and using only the most informative features or dimensions.\n",
    "  * Dimensionality Reduction: Techniques like Principal Component Analysis (PCA), t-distributed Stochastic Neighbor Embedding (t-SNE), and autoencoders can be used to reduce the number of dimensions while retaining most of the data's variability.\n",
    "  * Regularization: Methods like L1 (Lasso) and L2 (Ridge) regularization can be applied to penalize complex models, especially in regression contexts, to prevent overfitting.\n",
    "  * Sampling: Techniques like random sampling or clustering can be used to reduce data size, and thus dimensionality, without losing too much information.\n",
    "  * Ensemble Methods: Combining multiple models (like in random forests) can mitigate some of the risks of high dimensionality.\n",
    "  * Domain Knowledge: Incorporating knowledge from the domain of application can help in intelligently selecting features and understanding the data's underlying structure.\n",
    "* working with time series is difficult in deep learning, the curse of dimensionality comes into play if you try to learn on too many features with too little data\n",
    "* high computational / dataset burden for image timeseries / video prediction\n",
    "* in the remote sensing domain, due to dataset size, best to try smaller models first that convolve over the time dimension instead of the spatial dimensions\n",
    "* See https://developmentseed.org/blog/2023-06-29-time-travel-pixels and https://arxiv.org/abs/2207.13159 and https://arxiv.org/abs/2304.14065\n",
    "\n",
    "Vision Transformers and CNN alternatives\n",
    "* computational burden is also quadratic with number of self attention layers\n",
    "* ConvMixer is an alternative with lower computational burden, that is CNN and MLP based, available from the Keras team: https://huggingface.co/keras-io/convmixer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Architecture for image-to-image tasks, no fully connected layers because it doesn't compress feature information into a single category.\n",
    "Downsamples the image, then upsamples it into a segmentation mask.\n",
    "[Insert: Reference to U-Net Paper]\n",
    "\n",
    "\n",
    "ViT (Vision Transformer):\n",
    "Uses attention to focus on certain parts of images.\n",
    "Image split into patches, flattened, and transformed to embeddings.\n",
    "Pre-training and fine-tuning mechanisms are similar as they are for CNNs.\n",
    "Notable Paper: \"An image is worth 16x16 words: Transformers for image recognition at scale\".\n",
    "\n",
    "\n",
    "### Advanced Concepts: Deep learning architectures for segmentation and change detection in image time series\n",
    "\n",
    "ConvLSTM:\n",
    "Effective for time series data in computer vision.\n",
    "Knowledge Distillation:\n",
    "Teacher-student networks where the student (smaller) network tries to imitate the teacher (larger) network.\n",
    "SCCANet (Spatial and Spectral Channel Attention Network):\n",
    "An evolved form of U-Net with attention mechanisms.\n",
    "Stacked AutoEncoder (SAM?):\n",
    "Focuses on lossless compression.\n",
    "Comparison with PCA and deep autoencoders.\n",
    "Introduction of Variational autoencoders.\n",
    "RaVAEn: Unsupervised learning for change detection.\n",
    "Image Retrieval:\n",
    "Using vector databases for similarity computations.\n",
    "Image Captioning:\n",
    "[Insert: Brief Explanation and Examples]\n",
    "\n",
    "\n",
    "5. Generative Models\n",
    "\n",
    "Generative Adversarial Networks (GANs):\n",
    "Introduction to Generator and Discriminator networks.\n",
    "DCGANs (Deep Convolutional GANs).\n",
    "Applications:\n",
    "Generating human images (\"This person does not exist\").\n",
    "GAN-based style transfer.\n",
    "Age transitions using GANs.\n",
    "Super-resolution.\n",
    "Challenges:\n",
    "Formulating networks to \"compete\" and achieve desired results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatio-temporal transformer STT\n",
    "\n",
    "Spatial transformer\n",
    "\n",
    "Set and t models outperform spatial models\n",
    "\n",
    "Adversarial Label-efficient satellit image change detection\n",
    "- Manual inspection out of reach\n",
    "- Fully automatic solutions reach limitation. Irrelevant changes. Illumination occlusion, noise, blurring\n",
    "- Remove irrelevant changes with normalization, registration, \n",
    "- Reducing the variability\n",
    "- Or model the variability, remove with ML, DNNs SVMs\n",
    "- Hard to know how to model variability\n",
    "- Change detection method with active learning\n",
    "- Q&A model\n",
    "- Finds relevant exemplars to label using adversarial criterion that mixes diversity, representativity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change detection\n",
    "\n",
    "unet+MSOF\n",
    "\n",
    "FC+Siamese\n",
    "\n",
    "CLM: novel model for multi scale features and multi level context\n",
    "\n",
    "EMS-CDNet = uses partition unit for faster feature extraction and merging\n",
    "\n",
    "But it struggled with unstrictly registered datasets\n",
    "\n",
    "M2-CDNET addresses this. Deformable convolution learns offsets between given features to alleviate pixel displacement caused by registration errors. As the scale shrinks…\n",
    "\n",
    "Future work\n",
    "NERF, reduce registration errors and projection bias\n",
    "\n",
    "en.skyearth.org\n",
    "\n",
    "Zhangyj\n",
    "\n",
    "\n",
    "New talk: VHR change detection\n",
    "\n",
    "HCGMNet-CD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges in forecasting and detection for natural disasters\n",
    "\n",
    "Big data\n",
    "Labeling - imbalance, noisy labels\n",
    "Generalization\n",
    "Uncertainty in forecasting\n",
    "Stochastic nature, complex, non-linear interactions between earth system variables\n",
    "Effuse system for fire prediction in Europe assumes pine fuel everywhere\n",
    "\n",
    "Firecube: A daily data cube for the modeling and analysis of wildfires in Greece. Dataset is on zenodo\n",
    "\n",
    "Model types\n",
    "Pixel-wise = RF, XGBOOST\n",
    "Temporal pixel-wise: LSTM\n",
    "Spatiotemporal: ConvLSTM\n",
    "\n",
    "Showed convlstm does best, better than xgboost!\n",
    "\n",
    "Predictive accuracy isn;’t enough, fire brigade wants to know why. Explainable ai\n",
    "Can see what model is paying attention to, which variables and which times\n",
    "\n",
    "Bayesian neural network for uncertainty estimation\n",
    "\n",
    "Instead of scalar weights that are constant, replace them with distributions\n",
    "\n",
    "Used a Unet to segment wildfire forecast\n",
    "\n",
    "Unet doesn’t account for teleconnections. Memory effects from drought from last year\n",
    "\n",
    "TeleViT Teleconnection Vision transformer\n",
    "\n",
    "More resilient to long forecasting windows!\n",
    "\n",
    "62 to 61% AUPRC % instead of 62 to 58% for Unet ++ for long forecasting window\n",
    "\n",
    "Annotated interferograms of volcanos, the segmentation, labels for deformation, activity type\n",
    "\n",
    "ODC and ESDL are full data cube frameworks\n",
    "\n",
    "Datacube challenges, autocorrelation\n",
    "Difficult to model high dim data. East to overfit and need to regularize\n",
    "Computation issues\n",
    "\n",
    "Float 32 are 4 bytes each.\n",
    "\n",
    "Probabilistic ML hands on\n",
    "\n",
    "Discard fraction of most uncertain samples, inference on more certain samples should lead to lower loss. Shows that uncertainty estimate is more accurate if loss decreases with higher certainty sample inference\n",
    "\n",
    "SEASFIRE\n",
    "\n",
    "Uncertainty estimation\n",
    "\n",
    "1. Copernicus foundational models for dim reduction, multi modal data fusion\n",
    "2. Out of distribution generalization\n",
    "3. Earth as a graph,capture teleconnections. Ssls for earth as a graph, long term interactions\n",
    "4. Causality and physics, guided ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Structure of intro to deep learning\n",
    "* Convolution nns\n",
    "    * Convolution + subsampling + () + … + fully connected\n",
    "* Convolved feature creation graphic gif\n",
    "* Activation\n",
    "    * Simplifies learning, makes learning faster\n",
    "    * Avoids saturation issues\n",
    "* Subsampling, example: max pooling. Scale invariance. Params: type filter size, stride\n",
    "* Lacuna et al 1998 LeNet first CNN\n",
    "* Inception (GoogleNet 2014) convolutions with 3x3 and 5x5\n",
    "* Residuals = bypass some convolutions and add skip connections. ResNet\n",
    "* Allows very deep networks. 150 layers\n",
    "* Kaiming He 2016 CVPR “deep residual learning for image recognition”\n",
    "* DenseNet. Pushes residual connections by connecting everything to everything\n",
    "* Unet - arch that goes from images to images (paper?)\n",
    "* Shrinks image down that samples it back up\n",
    "* ViT\n",
    "    * Attention makes system focus on specific instances\n",
    "    * Split image into fixed patches, sizes\n",
    "    * Flatten image patches\n",
    "    * Create features, embeddings for patches\n",
    "    * Positional embeddings\n",
    "    * Feed embedding sequence to transformer encoder\n",
    "    * Pretrain it with image labels\n",
    "    * Retrain many samples, fine-tune\n",
    "    * An image is worth 16x16 words: Transformers for image recognition at scale\n",
    "* ConvLSTM for time series\n",
    "* Knowledge distillation, teacher student networks. smaller student network tries to approximate teacher\n",
    "* Show different date types, classification to instance segmentation\n",
    "* Segnet = cnn encoder decoder. Cone + batch norm + relu. Pooling. Upsampling. Softmax\n",
    "* Segnet different from net slightly. No bottleneck for segment. Segment is fully connected. 2017\n",
    "* Unet more popular than segment\n",
    "* 3D Unet possible. 4D Unet possible\n",
    "\n",
    "* SCCANet Spatial and Spectral Channel Attention Network\n",
    "* The size of intermediate layer sis very trial and error and empirical if it works\n",
    "* Basically still a Unet\n",
    "* There’s a lot of work that is basically just Unet or X Architecture that is published\n",
    "* Focus on using established architectures published from reliable sources in easy to use libraries/ml frameworks\n",
    "\n",
    "Stacked AutoEncoder (SAM?)\n",
    "* Lossless compression\n",
    "* Comparison to PCA, retrieving input in the output\n",
    "* Even with 730 or 30 intermediate dimensions\n",
    "* Deep auto encoders\n",
    "* Intermediate representations are decoded\n",
    "* Network extracts higher level features \n",
    "* Variational auto encoders\n",
    "    * Encoding distribution is regularized during training to ensure latent space can generate new data\n",
    "    * Difference between auto encoder to variational auto encoder\n",
    "* RaVAEn unsupervised learning for change detection\n",
    "    * Detecting floods without labels\n",
    "    * Show many examples of unlabeled flooding\n",
    "    * Time series of images that include after event image\n",
    "    * Is it necessary to order the after event image as the final image in the time series?\n",
    "    * Need geographic representation\n",
    "    * Used Sentinel-2. 4 before event 1 after event\n",
    "    * Cosine baseline vs cosine embeddings?\n",
    "    * Cosine embedding matched the true label more\n",
    "* Image retrieval - vector database\n",
    "    * Query image to do similarity computation\n",
    "* Image captioning\n",
    "* Generators?\n",
    "    * GANS start from random inputs and generate images\n",
    "    * Adversarial part is that two networks compete. One generates images, another network generates predictions if results from counterpart are fake or not\n",
    "    * Generator network and discriminator network\n",
    "    * DCGANs, deep convolutional generative adversarial networks\n",
    "    * This person does not exist, old news, better models now\n",
    "    * Gan based style transfer\n",
    "    * Gan transitions (aging)\n",
    "    * No ground truth!\n",
    "    * Gans for super resolution. Nothing new in terms of arch. CNNS, max pooling, RELU, sigmoid, etc.\n",
    "    * Problem statement with GANs is formulating networks to connect and battle each other to arrive at a specific objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeoFM SAM notes\n",
    "\n",
    "Presto\n",
    "* Operates on pixel time series of multi sensors, multi channel\n",
    "* Channel group embedding, positional, embedding for lagoon, and month embedding\n",
    "* Somewhat marginal but clear improvement over other approach (also time series approach)\n",
    "* Geographic representation across hemispheres and ecoregion\n",
    "* Used dynamic world as an input and to stratify\n",
    "* Presto can be used as feature extractor (for random forest and regression) or fine-tuning the encoder and linear transformation\n",
    "* Main comparison is to Task Informed Meta Learning\n",
    "* Presto is fully self supervised, computationally more efficient than image based approaches\n",
    "* Also beats SatMAE\n",
    "* Also tested fully supervised presto to measure effect of arch vs the self supervision training regime\n",
    "* Didn’t really understand all presto tables, especially table 6\n",
    "* Self supervised presto beats fully supervised presto for particular tasks\n",
    "\n",
    "\n",
    "MedSAM\n",
    "* Strong qualitative improvement in fuzzy boundary objects\n",
    "* Text encoding with CLIP, todo read\n",
    "* SAM can generate object, part, and subpart masks, 4 each. Todo enable?\n",
    "* The mode matters a lot. What’s best for wall to wall mapping…. Segment anything mode has no semantic labels\n",
    "* What’s best for fuzzy objects? Probably box? Sometimes multi point? When does part and subpart matter?\n",
    "* Box == less trial and error\n",
    "* They freeze the encoder\n",
    "* Only mask decoder fine tuned\n",
    "* Pregenerated all training image embeddings\n",
    "* Image encoder resizes images by default to 3x1024x1024, yuck?\n",
    "Through the fine-tuning of SAM on medical image datasets, MedSAM has greatly enhanced the model’s ability to identify challenging segmentation targets. Specifically, MedSAM has demonstrated three significant improvements over the pre-trained SAM. Firstly, MedSAM has improved the model’s ability to identify small objects, even when multiple segmentation targets are present within the bounding box prompt. Secondly, the model has shown more robustness towards weak boundaries in various modalities, such as lesion and left ventricle segmen- tation in ultrasound and brain MR images, respectively. Finally, MedSAM has effectively reduced interference from high-contrast objects surrounding the seg- mentation target, resulting in fewer outliers. \n",
    "* Dice similarity saw huge improvement for nearly all categories, table 1 and 2\n",
    "* Scribble based prompt, interesting idea\n",
    "https://github.com/MIC-DKFZ/napari-sam \n",
    "\n",
    "Sam-adapter-med\n",
    "* Different group almost same time\n",
    "* Different “fine-tuning” method with prompting. Throws away sAM decoder and encoder is frozen\n",
    "* Uses two MLP layers to supply prompts to the encoder\n",
    "* They seem to get great results, unclear if they are not using any prompts at all? Seems like these predictions are promptless.\n",
    "* So better results on specific tasks with adapter fine tuning for each task. Less generalizability for each adapted model and you lose promptability?\n",
    "\n",
    "Personalize-SAM\n",
    "- Lightning fast finetuning variant PerSAM-F\n",
    "- Training free variant PerSAM using one example image and mask\n",
    "- Personalizing sam to segment unique visual concepts (your pet dog)\n",
    "- Learns the best mask scale for a particular problem, handling the object/part/subpart selection choice that SAM offers\n",
    "- This seems unique from the adapter and traditional fine tuning approach\n",
    "- “achieves leading performance on our annotated PerSeg dataset” lol\n",
    "- Mainly compares to seggpt no their own dataset, not other segmentation approaches like sam adapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ds_book]",
   "language": "python",
   "name": "conda-env-ds_book-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
