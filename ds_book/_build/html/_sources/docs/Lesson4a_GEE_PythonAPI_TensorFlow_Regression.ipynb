{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SHAc5qbiR8l"
      },
      "source": [
        "# Regression using TensorFlow with Google Earth Engine Python API\n",
        "\n",
        "This tutorial demonstrates how to train a model to predict continuous output (regression) from geospatial data. To do this, we will use the impervious surface area dataset from [NLCD](https://www.mrlc.gov/data) and a Landsat 8 composite. Both datasets will be acquired from earth engine. Using the Keras sequential API, we will show how to train a [U-net](https://arxiv.org/abs/1505.04597) model to predict impervious surface area percentage for each pixel in an image. This tutorial is an adaptation of this [example](https://developers.google.com/earth-engine/guides/tf_examples#regression-with-an-fcnn).\n",
        "\n",
        "This tutorial covers:\n",
        "\n",
        "1.   Generating and exporting training/testing patches from Google Earth Engine (henceforth referred to as Earth Engine or EE).\n",
        "3.   Using an iterator to supply batches furing training and testing.\n",
        "3.   Training the regression model and saving it.\n",
        "4.   Generating predictions with the trained model and plotting them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MJ4kW1pEhwP"
      },
      "source": [
        "## Imports and authentication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neIa46CpciXq"
      },
      "outputs": [],
      "source": [
        "# Cloud authentication.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jat01FEoUMqg"
      },
      "outputs": [],
      "source": [
        "# Import, authenticate and initialize the Earth Engine library.\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6lce1eu0C2E"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "#print(tf.__version__)\n",
        "from tensorflow.keras import layers, losses, models, metrics, optimizers\n",
        "\n",
        "import folium, glob, json, tifffile\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from random import randrange\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMt-vVkqRCM3"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "# set your root directory and tiled data folders\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    # mount google drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    root_dir = '/content/gdrive/My Drive/tf-eo-devseed-2/'\n",
        "    workshop_dir = '/content/gdrive/My Drive/tf-eo-devseed-workshop-2'\n",
        "    dirs = [root_dir, workshop_dir]\n",
        "    for d in dirs:\n",
        "        if not os.path.exists(d):\n",
        "            os.makedirs(d)\n",
        "    print('Running on Colab')\n",
        "else:\n",
        "    root_dir = os.path.abspath(\"./data/tf-eo-devseed-2\")\n",
        "    workshop_dir = os.path.abspath('./tf-eo-devseed-workshop-2')\n",
        "    print(f'Not running on Colab, data needs to be downloaded locally at {os.path.abspath(root_dir)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFyH89P4RFbj"
      },
      "outputs": [],
      "source": [
        "%cd $root_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmfKLl9XcnGJ"
      },
      "source": [
        "## Set global parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psz7wJKalaoj"
      },
      "outputs": [],
      "source": [
        "# Specify names locations for outputs in Cloud Storage.\n",
        "FOLDER = 'fcnn-demo'\n",
        "PREDICTIONS = 'predictions'\n",
        "TEST_PATCHES = 'test_patches'\n",
        "TRAINING_BASE = 'training_patches'\n",
        "VAL_BASE = 'val_patches'\n",
        "\n",
        "# Specify inputs (Landsat bands) to the model and the response variable.\n",
        "opticalBands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
        "thermalBands = ['B10', 'B11']\n",
        "BANDS = opticalBands + thermalBands\n",
        "RESPONSE = 'impervious'\n",
        "FEATURES = BANDS + [RESPONSE]\n",
        "\n",
        "# Specify the size and shape of patches expected by the model.\n",
        "KERNEL_SIZE = 256\n",
        "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
        "COLUMNS = [\n",
        "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
        "]\n",
        "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
        "\n",
        "# Sizes of the training and evaluation datasets.\n",
        "TRAIN_SIZE = 16000\n",
        "VAL_SIZE = 8000\n",
        "\n",
        "# Specify model training parameters.\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 10\n",
        "BUFFER_SIZE = 2000\n",
        "OPTIMIZER = 'SGD'\n",
        "LOSS = 'MeanSquaredError'\n",
        "METRICS = ['RootMeanSquaredError']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIoL0f3GXfNl"
      },
      "outputs": [],
      "source": [
        "dirs = [f\"{FOLDER}\", f\"{FOLDER}/{PREDICTIONS}\",  f\"{FOLDER}/{TEST_PATCHES}\"]\n",
        "for d in dirs:\n",
        "  if not os.path.exists(d):\n",
        "    os.makedirs(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IO52qQlUUc-D"
      },
      "outputs": [],
      "source": [
        "!ls fcnn-demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgoDc7Hilfc4"
      },
      "source": [
        "# Imagery\n",
        "\n",
        "Collect and process the input imagery (cloud masking, compositing).  Display the composite."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IlgXu-vcUEY"
      },
      "outputs": [],
      "source": [
        "# Use Landsat 8 surface reflectance data.\n",
        "l8sr = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\n",
        "\n",
        "# Cloud masking function.\n",
        "def maskL8sr(image):\n",
        "  cloudShadowBitMask = ee.Number(2).pow(3).int()\n",
        "  cloudsBitMask = ee.Number(2).pow(5).int()\n",
        "  qa = image.select('pixel_qa')\n",
        "  mask1 = qa.bitwiseAnd(cloudShadowBitMask).eq(0).And(\n",
        "    qa.bitwiseAnd(cloudsBitMask).eq(0))\n",
        "  mask2 = image.mask().reduce('min')\n",
        "  mask3 = image.select(opticalBands).gt(0).And(\n",
        "          image.select(opticalBands).lt(10000)).reduce('min')\n",
        "  mask = mask1.And(mask2).And(mask3)\n",
        "  return image.select(opticalBands).divide(10000).addBands(\n",
        "          image.select(thermalBands).divide(10).clamp(273.15, 373.15)\n",
        "            .subtract(273.15).divide(100)).updateMask(mask)\n",
        "\n",
        "# The image input data is a cloud-masked median composite.\n",
        "image = l8sr.filterDate('2015-01-01', '2017-12-31').map(maskL8sr).median()\n",
        "\n",
        "# Use folium to visualize the imagery.\n",
        "mapid = image.getMapId({'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.3})\n",
        "map = folium.Map(location=[38., -122.5])\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='median composite',\n",
        "  ).add_to(map)\n",
        "\n",
        "mapid = image.getMapId({'bands': ['B10'], 'min': 0, 'max': 0.5})\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='thermal',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHznnctkJsZJ"
      },
      "source": [
        "Collect the labels (impervious surface area (in fraction of a pixel)) and display."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0wHDyxVirec"
      },
      "outputs": [],
      "source": [
        "nlcd = ee.Image('USGS/NLCD/NLCD2016').select('impervious')\n",
        "nlcd = nlcd.divide(100).float()\n",
        "\n",
        "mapid = nlcd.getMapId({'min': 0, 'max': 1})\n",
        "map = folium.Map(location=[38., -122.5])\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='nlcd impervious',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTS7_ZzPDhhg"
      },
      "source": [
        "Now we will combine the Landsat composite and NLCD impervious surface raster into a single stacked image array. From that, we will break the image down into patches with width and height of 256 pixels.\n",
        "\n",
        "To convert the EE multi-band image collection to an image array, we use [`neighborhoodToArray()`](https://developers.google.com/earth-engine/api_docs#eeimageneighborhoodtoarray), then proceed to sample the image at selective areas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGHYsdAOipa4"
      },
      "outputs": [],
      "source": [
        "featureStack = ee.Image.cat([\n",
        "  image.select(BANDS),\n",
        "  nlcd.select(RESPONSE)\n",
        "]).float()\n",
        "\n",
        "list = ee.List.repeat(1, KERNEL_SIZE)\n",
        "lists = ee.List.repeat(list, KERNEL_SIZE)\n",
        "kernel = ee.Kernel.fixed(KERNEL_SIZE, KERNEL_SIZE, lists)\n",
        "\n",
        "arrays = featureStack.neighborhoodToArray(kernel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4djSxBRG2el"
      },
      "source": [
        "Use some pre-made geometries to sample the stack in strategic locations.  Specifically, these are hand-made polygons in which to take the 256x256 samples.  Display the sampling polygons on a map, red for training polygons, blue for evaluation.\n",
        "\n",
        "We will strategiclly sample the imagery using some diverse, representative geometries. The red geometries plotted below are for training, while the blue are for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ure_WaD0itQY"
      },
      "outputs": [],
      "source": [
        "trainingPolys = ee.FeatureCollection('projects/google/DemoTrainingGeometries')\n",
        "valPolys = ee.FeatureCollection('projects/google/DemoEvalGeometries')\n",
        "\n",
        "polyImage = ee.Image(0).byte().paint(trainingPolys, 1).paint(valPolys, 2)\n",
        "polyImage = polyImage.updateMask(polyImage)\n",
        "\n",
        "mapid = polyImage.getMapId({'min': 1, 'max': 2, 'palette': ['red', 'blue']})\n",
        "map = folium.Map(location=[38., -100.], zoom_start=5)\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='training polygons',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkzjoOwSJrft"
      },
      "outputs": [],
      "source": [
        "# How many polygons do we have in total?\n",
        "trainingPolys.size().getInfo(), valPolys.size().getInfo()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV890gPHeZqz"
      },
      "source": [
        "# Sampling\n",
        "\n",
        "Now we will use those geometries to extract samples from the stacked image array. Within each geometry we take a 256x256 neighborhood of pixels around several points, shard them to prevent memory error and then collect them into a single tfrecord. This is done for the training and validation data separately.\n",
        "\n",
        "Note: for brevity's sake in this tutorial, we are only sampling from 2 training geometries and 1 validation geometry. You can revise `range(2): #range(trainingPolys.size().getInfo())` and the corresponding line for the validation set if you wish to use all geometries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "FyRpvwENxE-A"
      },
      "outputs": [],
      "source": [
        " # Convert the feature collections to lists for iteration.\n",
        "trainingPolysList = trainingPolys.toList(trainingPolys.size())\n",
        "valPolysList = valPolys.toList(valPolys.size())\n",
        "\n",
        "# These numbers determined experimentally.\n",
        "n = 200 # Number of shards in each polygon.\n",
        "N = 2000 # Total sample size in each polygon.\n",
        "\n",
        "# Export all the training data (in many pieces), with one task\n",
        "# per geometry.\n",
        "for g in range(2): #range(trainingPolys.size().getInfo()):\n",
        "  geomSample = ee.FeatureCollection([])\n",
        "  for i in range(n):\n",
        "    sample = arrays.sample(\n",
        "      region = ee.Feature(trainingPolysList.get(g)).geometry(),\n",
        "      scale = 30,\n",
        "      numPixels = N / n, # Size of the shard.\n",
        "      seed = i,\n",
        "      tileScale = 8\n",
        "    )\n",
        "    geomSample = geomSample.merge(sample)\n",
        "    #print(geomSample)\n",
        "\n",
        "  desc = TRAINING_BASE + '_g' + str(g)\n",
        "  task = ee.batch.Export.table.toDrive(\n",
        "    collection = geomSample,\n",
        "    description = desc,\n",
        "    folder = FOLDER,\n",
        "    fileNamePrefix = desc,\n",
        "    fileFormat = 'TFRecord',\n",
        "    selectors = BANDS + [RESPONSE]\n",
        "  )\n",
        "  print(FOLDER, desc)\n",
        "  task.start()\n",
        "\n",
        "# Export all the evaluation data.\n",
        "for g in range(1): #range(valPolys.size().getInfo()):\n",
        "  geomSample = ee.FeatureCollection([])\n",
        "  for i in range(n):\n",
        "    sample = arrays.sample(\n",
        "      region = ee.Feature(valPolysList.get(g)).geometry(),\n",
        "      scale = 30,\n",
        "      numPixels = N / n,\n",
        "      seed = i,\n",
        "      tileScale = 8\n",
        "    )\n",
        "    geomSample = geomSample.merge(sample)\n",
        "\n",
        "  desc = VAL_BASE + '_g' + str(g)\n",
        "  task = ee.batch.Export.table.toDrive(\n",
        "    collection = geomSample,\n",
        "    description = desc,\n",
        "    folder = FOLDER,\n",
        "    fileNamePrefix = desc,\n",
        "    fileFormat = 'TFRecord',\n",
        "    selectors = BANDS + [RESPONSE]\n",
        "  )\n",
        "  print(FOLDER, desc)\n",
        "  task.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWXrvBE4607G"
      },
      "source": [
        "# Training data\n",
        "\n",
        "Now that we have exported a TFRecord from Earth Engine, let's load it into a `tf.data.Dataset`.  Unfortunately, there isn't a path to load the data directly from Earth Engine into a `tf.data.Dataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWZ0UXCVMyJP"
      },
      "outputs": [],
      "source": [
        "def parse_tfrecord(example_proto):\n",
        "  \"\"\"The parsing function.\n",
        "  Read a serialized example into the structure defined by FEATURES_DICT.\n",
        "  Args:\n",
        "    example_proto: a serialized Example.\n",
        "  Returns:\n",
        "    A dictionary of tensors, keyed by feature name.\n",
        "  \"\"\"\n",
        "  return tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
        "\n",
        "\n",
        "def to_tuple(inputs):\n",
        "  \"\"\"Function to convert a dictionary of tensors to a tuple of (inputs, outputs).\n",
        "  Turn the tensors returned by parse_tfrecord into a stack in HWC shape.\n",
        "  Args:\n",
        "    inputs: A dictionary of tensors, keyed by feature name.\n",
        "  Returns:\n",
        "    A tuple of (inputs, outputs).\n",
        "  \"\"\"\n",
        "  inputsList = [inputs.get(key) for key in FEATURES]\n",
        "  stacked = tf.stack(inputsList, axis=0)\n",
        "  # Convert from CHW to HWC\n",
        "  stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "  return stacked[:,:,:len(BANDS)], stacked[:,:,len(BANDS):]\n",
        "\n",
        "\n",
        "def get_dataset(pattern):\n",
        "  \"\"\"Function to read, parse and format to tuple a set of input tfrecord files.\n",
        "  Get all the files matching the pattern, parse and convert to tuple.\n",
        "  Args:\n",
        "    pattern: A file pattern to match in a Cloud Storage bucket.\n",
        "  Returns:\n",
        "    A tf.data.Dataset\n",
        "  \"\"\"\n",
        "  glob = tf.io.gfile.glob(pattern)\n",
        "  dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
        "  dataset = dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
        "  dataset = dataset.map(to_tuple, num_parallel_calls=5)\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg1fa18336D2"
      },
      "source": [
        "Parse the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm0qRF0fAYcC"
      },
      "outputs": [],
      "source": [
        "def get_training_dataset():\n",
        "\t\"\"\"Get the preprocessed training dataset\n",
        "  Returns:\n",
        "    A tf.data.Dataset of training data.\n",
        "  \"\"\"\n",
        "\tglobb = f\"{FOLDER}/{TRAINING_BASE}/*\"\n",
        "\tdataset = get_dataset(globb)\n",
        "\tdataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\treturn dataset\n",
        "\n",
        "training = get_training_dataset()\n",
        "\n",
        "#print(iter(training.take(1)).next())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-cQO5RL6vob"
      },
      "source": [
        "# Validation data\n",
        "\n",
        "Parse the validation dataset.  Note that the validation dataset has a batch size of 1 which is different from the training dataset. Another distinction is that the validation dataset is not shuffled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieKTCGiJ6xzo"
      },
      "outputs": [],
      "source": [
        "def get_val_dataset():\n",
        "\t\"\"\"Get the preprocessed validation dataset\n",
        "  Returns:\n",
        "    A tf.data.Dataset of validation data.\n",
        "  \"\"\"\n",
        "\tglobb = f\"{FOLDER}/{VAL_BASE}/*\"\n",
        "\tdataset = get_dataset(globb)\n",
        "\tdataset = dataset.batch(1).repeat()\n",
        "\treturn dataset\n",
        "\n",
        "validation = get_val_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JIE7Yl87lgU"
      },
      "source": [
        "# Model\n",
        "\n",
        "For our model, we will use a Keras implementation of the U-Net model, and provide the network with 256x256 pixel image patches as input. The output will be probabilities for each pixel (a continuous output).\n",
        "\n",
        "As this is a regression problem, we apply mean squared error as our loss function. As well, we implement a saturating activation function to address any artifacts that are produced from squashing the range of values to [0,1] (something we need to do to make a sensible measurement of impervious surface fraction per pixel)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsnnnz56yS3l"
      },
      "outputs": [],
      "source": [
        "def conv_block(input_tensor, num_filters):\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\treturn encoder\n",
        "\n",
        "def encoder_block(input_tensor, num_filters):\n",
        "\tencoder = conv_block(input_tensor, num_filters)\n",
        "\tencoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
        "\treturn encoder_pool, encoder\n",
        "\n",
        "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
        "\tdecoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
        "\tdecoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\treturn decoder\n",
        "\n",
        "def get_model():\n",
        "\tinputs = layers.Input(shape=[None, None, len(BANDS)]) # 256\n",
        "\tencoder0_pool, encoder0 = encoder_block(inputs, 32) # 128\n",
        "\tencoder1_pool, encoder1 = encoder_block(encoder0_pool, 64) # 64\n",
        "\tencoder2_pool, encoder2 = encoder_block(encoder1_pool, 128) # 32\n",
        "\tencoder3_pool, encoder3 = encoder_block(encoder2_pool, 256) # 16\n",
        "\tencoder4_pool, encoder4 = encoder_block(encoder3_pool, 512) # 8\n",
        "\tcenter = conv_block(encoder4_pool, 1024) # center\n",
        "\tdecoder4 = decoder_block(center, encoder4, 512) # 16\n",
        "\tdecoder3 = decoder_block(decoder4, encoder3, 256) # 32\n",
        "\tdecoder2 = decoder_block(decoder3, encoder2, 128) # 64\n",
        "\tdecoder1 = decoder_block(decoder2, encoder1, 64) # 128\n",
        "\tdecoder0 = decoder_block(decoder1, encoder0, 32) # 256\n",
        "\toutputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(decoder0)\n",
        "\n",
        "\tmodel = models.Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "\tmodel.compile(\n",
        "\t\toptimizer=optimizers.get(OPTIMIZER),\n",
        "\t\tloss=losses.get(LOSS),\n",
        "\t\tmetrics=[metrics.get(metric) for metric in METRICS])\n",
        "\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu_E7OTDBCoS"
      },
      "source": [
        "# Training the model\n",
        "\n",
        "Now we train the compiled network by calling `.fit()`.  We will train for a minimal 10 epochs, which works for a demo.  However, the model may improve with more iteration, which can be experimented with [hyperparameter tuning](https://cloud.google.com/ml-engine/docs/tensorflow/using-hyperparameter-tuning) and implementation specifically of early stopping mechanisms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzzaWxOhSxBy"
      },
      "outputs": [],
      "source": [
        "m = get_model()\n",
        "\n",
        "m.fit(\n",
        "    x=training,\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=int(TRAIN_SIZE / BATCH_SIZE),\n",
        "    validation_data=validation,\n",
        "    validation_steps=VAL_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9GrXvK7i6B_"
      },
      "source": [
        "Let's save the trained model to file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11aiXtSsfBSC"
      },
      "outputs": [],
      "source": [
        "save_model_path = os.path.join(f\"{FOLDER}/model_out_batch_{BATCH_SIZE}_ep{EPOCHS}/\")\n",
        "if (not os.path.isdir(save_model_path)):\n",
        "  os.mkdir(save_model_path)\n",
        "m.save(save_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2XrwZHp66j4"
      },
      "source": [
        "After saving the trained model, you may want to run predictions later. Let's try loading our saved model to show how it can be reused without having to retrain the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RJpNfEUS1qp"
      },
      "outputs": [],
      "source": [
        "# Load a trained model.\n",
        "MODEL_DIR = f\"{FOLDER}/model_out_batch_{BATCH_SIZE}_ep{EPOCHS}/\"\n",
        "m = tf.keras.models.load_model(MODEL_DIR)\n",
        "m.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1ySNup0xCqN"
      },
      "source": [
        "# Prediction\n",
        "\n",
        "Now, let's make predictions for a new area to see how the model generalizes to new data. Bear in mind, the model was trained on data in the US because that is where the labels were available, but let's see the trained model applies to a region of Lima, Peru.\n",
        "\n",
        "Again, we will use a defined geometry to process and export imagery from Earth Engine in TFRecord format. Then we'll use our trained model to predict impervious surface area percentages on the new imagery and writ the predictions to both a TFRecord file and image patches (true color and prediction).\n",
        "\n",
        "We separate the image export from the predict function because the export only needs to happen once, but perhaps you'll experiment with the model setup and run new predictions several times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3WDAa-RUpXP"
      },
      "outputs": [],
      "source": [
        "def doExport(out_image_base, kernel_buffer, region):\n",
        "  \"\"\"Run the image export task.  Block until complete.\n",
        "  \"\"\"\n",
        "\n",
        "  task = ee.batch.Export.image.toDrive(\n",
        "    image = image.select(BANDS),\n",
        "    description = out_image_base,\n",
        "    folder = FOLDER,\n",
        "    fileNamePrefix = f\"{out_image_base}\",\n",
        "    region = region.getInfo()['coordinates'],\n",
        "    scale = 30,\n",
        "    fileFormat = 'TFRecord',\n",
        "    maxPixels = 1e10,\n",
        "    formatOptions = {\n",
        "      'patchDimensions': KERNEL_SHAPE,\n",
        "      'kernelSize': kernel_buffer,\n",
        "      'compressed': True,\n",
        "      'maxFileSize': 104857600\n",
        "    }\n",
        "  )\n",
        "\n",
        "  task.start()\n",
        "\n",
        "  # Block until the task completes.\n",
        "  print('Running image export to Cloud Storage...')\n",
        "  import time\n",
        "  while task.active():\n",
        "    time.sleep(30)\n",
        "\n",
        "  # Error condition\n",
        "  if task.status()['state'] != 'COMPLETED':\n",
        "    print('Error with image export.')\n",
        "  else:\n",
        "    print('Image export completed.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zb_9_FflygVw"
      },
      "outputs": [],
      "source": [
        "def doPrediction(out_image_base, kernel_buffer):\n",
        "  \"\"\"Perform inference on exported imagery, upload to Earth Engine.\n",
        "  \"\"\"\n",
        "\n",
        "  print('Looking for TFRecord files...')\n",
        "\n",
        "  # Get a list of all the files in the output bucket.\n",
        "  filesList = glob.glob(f\"{FOLDER}/*\")\n",
        "\n",
        "  # Get only the files generated by the image export.\n",
        "  exportFilesList = [s for s in filesList if out_image_base in s]\n",
        "  print(\"exportFilesList: \", exportFilesList)\n",
        "\n",
        "  # Get the list of image files and the JSON mixer file.\n",
        "  imageFilesList = []\n",
        "  jsonFile = None\n",
        "  for f in exportFilesList:\n",
        "    if f.endswith('.tfrecord.gz'):\n",
        "      imageFilesList.append(f)\n",
        "    elif f.endswith('.json'):\n",
        "      jsonFile = f\n",
        "  #jsonFile = f\"{out_json_base}-mixer.json\"\n",
        "  # Make sure the files are in the right order.\n",
        "  imageFilesList.sort()\n",
        "  pprint(imageFilesList)\n",
        "  print(\"jsonFile: \", jsonFile)\n",
        "\n",
        "  # Load the contents of the mixer file to a JSON object.\n",
        "  jsonText = !cat {jsonFile}\n",
        "  # Get a single string w/ newlines from the IPython.utils.text.SList\n",
        "  mixer = json.loads(jsonText.nlstr)\n",
        "  pprint(mixer)\n",
        "  patches = mixer['totalPatches']\n",
        "\n",
        "  # Get set up for prediction.\n",
        "  x_buffer = int(kernel_buffer[0] / 2)\n",
        "  y_buffer = int(kernel_buffer[1] / 2)\n",
        "\n",
        "  buffered_shape = [\n",
        "      KERNEL_SHAPE[0] + kernel_buffer[0],\n",
        "      KERNEL_SHAPE[1] + kernel_buffer[1]]\n",
        "\n",
        "  imageColumns = [\n",
        "    tf.io.FixedLenFeature(shape=buffered_shape, dtype=tf.float32)\n",
        "      for k in BANDS\n",
        "  ]\n",
        "\n",
        "  print(BANDS)\n",
        "\n",
        "  imageFeaturesDict = dict(zip(BANDS, imageColumns))\n",
        "\n",
        "  def parse_image(example_proto):\n",
        "    print(tf.io.parse_single_example(example_proto, imageFeaturesDict))\n",
        "    return tf.io.parse_single_example(example_proto, imageFeaturesDict)\n",
        "\n",
        "  def toTupleImage(inputs):\n",
        "    inputsList = [inputs.get(key) for key in BANDS]\n",
        "    stacked = tf.stack(inputsList, axis=0)\n",
        "    stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "    return stacked\n",
        "\n",
        "   # Create a dataset from the TFRecord file(s) in Cloud Storage.\n",
        "  imageDataset = tf.data.TFRecordDataset(imageFilesList, compression_type='GZIP')\n",
        "  imageDataset = imageDataset.map(parse_image, num_parallel_calls=5)\n",
        "  imageDataset = imageDataset.map(toTupleImage).batch(1)\n",
        "  #print(\"imageDataset: \", imageDataset)\n",
        "\n",
        "  imageDataset1 = imageDataset.batch(1)\n",
        "\n",
        "  for i, image in zip(range(patches), imageDataset1):  # only take first element of dataset\n",
        "    t_image_array = np.array(image)\n",
        "    t_image_array = t_image_array.squeeze()\n",
        "    t_image_array = t_image_array[\n",
        "        x_buffer:x_buffer+KERNEL_SIZE, y_buffer:y_buffer+KERNEL_SIZE]\n",
        "    t_image_array_blue = t_image_array[:,:,1]\n",
        "    t_image_array_green = t_image_array[:,:,2]\n",
        "    t_image_array_red = t_image_array[:,:,3]\n",
        "    t_image_array = np.dstack([t_image_array_red, t_image_array_green, t_image_array_blue])\n",
        "    t_image_array *= 255\n",
        "    t_image_array = t_image_array.astype(np.uint8)\n",
        "    tifffile.imsave(f\"{FOLDER}/{TEST_PATCHES}/patch_test_{i}.tif\", t_image_array)\n",
        "\n",
        "\n",
        "  # Perform inference.\n",
        "  print('Running predictions...')\n",
        "  predictions = m.predict(imageDataset, steps=patches, verbose=1)\n",
        "\n",
        "  for i, prediction in zip(range(len(predictions)), predictions):\n",
        "    predictionPatch = prediction[\n",
        "        x_buffer:x_buffer+KERNEL_SIZE, y_buffer:y_buffer+KERNEL_SIZE]\n",
        "    p_image_array = np.array(predictionPatch)\n",
        "    tifffile.imsave(f\"{FOLDER}/{PREDICTIONS}/patch_pred_{i}.tif\", p_image_array)\n",
        "\n",
        "  print('Writing predictions...')\n",
        "  out_image_file = f\"{FOLDER}/{out_image_base}.TFRecord\"\n",
        "  writer = tf.io.TFRecordWriter(out_image_file)\n",
        "  patches = 0\n",
        "  for predictionPatch in predictions:\n",
        "    #print('Writing patch ' + str(patches) + '...')\n",
        "    predictionPatch = predictionPatch[\n",
        "        x_buffer:x_buffer+KERNEL_SIZE, y_buffer:y_buffer+KERNEL_SIZE]\n",
        "\n",
        "    # Create an example.\n",
        "    example = tf.train.Example(\n",
        "      features=tf.train.Features(\n",
        "        feature={\n",
        "          'impervious': tf.train.Feature(\n",
        "              float_list=tf.train.FloatList(\n",
        "                  value=predictionPatch.flatten()))\n",
        "        }\n",
        "      )\n",
        "    )\n",
        "    # Write the example.\n",
        "    writer.write(example.SerializeToString())\n",
        "    patches += 1\n",
        "  print(out_image_file)\n",
        "  writer.close()\n",
        "  return patches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZqlymOehnQO"
      },
      "source": [
        "Let's supply our area of interest to make predictions for. We also provide some string parameters for file naming and finally, the shape for the image outputs. On that, the model can accept larger dimensions than 256x256 (which it was trained on) provided that they are uniform in width and height (note that we didn't specify an input shape in the first layer of the network) but at some point as the dimensions increase a memory ceiling will be encountered ([reference](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf)). So, proceed with caution on that. We will try one technique here to address the common issue of edge artifacts. Specifically, we will buffer our images during prediction using a 128x128 kernel, which pads the image with an additional 64 pixels on both width and height, and then clip the prediction down to the original central region of 256x256 pixels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoGESVQ_OvHC"
      },
      "outputs": [],
      "source": [
        "# Output assets\n",
        "\n",
        "# Base file name to use for TFRecord files and assets.\n",
        "lima_image_base = 'FCNN_demo_lima_384_'\n",
        "# Half this will extend on the sides of each patch.\n",
        "lima_kernel_buffer = [128, 128]\n",
        "# Lima [-77.133581 -12.164808 -76.876993 -11.93859]\n",
        "lima_region = ee.Geometry.Polygon(\n",
        "        [[[-77.133581, -11.93859],\n",
        "          [-77.133581, -12.164808],\n",
        "          [-76.876993, -12.164808],\n",
        "          [-76.876993, -11.93859]]], None, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLNEOLkXWvSi"
      },
      "outputs": [],
      "source": [
        "# Run the export.\n",
        "doExport(lima_image_base, lima_kernel_buffer, lima_region)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxACnxKFrQ_J"
      },
      "outputs": [],
      "source": [
        "# Run the prediction.\n",
        "patches = doPrediction(lima_image_base, lima_kernel_buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj_G9OZ1xH6K"
      },
      "source": [
        "# Display the output\n",
        "\n",
        "Let's take a look at randomly indexed samples from our prediction output. We also will check the minimum and maximum values (impervious surface area percentages) in the prediction image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ju2l6gRKwnv6"
      },
      "outputs": [],
      "source": [
        "fig, (ax0, ax1) = plt.subplots(1, 2, sharey=True, sharex=True, figsize=(12, 6))\n",
        "\n",
        "index_random = randrange(patches)\n",
        "print(index_random)\n",
        "\n",
        "ax0.imshow(np.array(Image.open(f\"{FOLDER}/{TEST_PATCHES}/patch_test_{index_random}.tif\")))\n",
        "ax0.set_xlabel(f'RGB').set_fontsize(9)\n",
        "ax1.imshow(np.array(Image.open(f\"{FOLDER}/{PREDICTIONS}/patch_pred_{index_random}.tif\")))\n",
        "ax1.set_xlabel(f'Predicted impervious surface area percentage').set_fontsize(9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oW6CHqsYcn3W"
      },
      "outputs": [],
      "source": [
        "image_test = np.array(Image.open(f\"{FOLDER}/{PREDICTIONS}/patch_pred_{index_random}.tif\"))\n",
        "image_test.min(), image_test.max()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
