

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Introduction to machine learning, neural networks and deep learning &#8212; Deep learning with TensorFlow</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/Lesson1a_Intro_ML_NN_DL';</script>
    <link rel="shortcut icon" href="../_static/ds.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Introduction to Google Colab" href="Lesson2a_Intro_to_Google_Colab.html" />
    <link rel="prev" title="Deep Learning with TensorFlow: Tutorials for modeling LULC." href="index.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/ds.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/ds.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Introduction to machine learning, neural networks and deep learning</a></li>

<li class="toctree-l1"><a class="reference internal" href="Lesson2a_Intro_to_Google_Colab.html">Introduction to Google Colab</a></li>


<li class="toctree-l1"><a class="reference internal" href="Lesson2b_Intro_TensorFlow_Keras.html">Introduction to TensorFlow 2 and Keras</a></li>
<li class="toctree-l1"><a class="reference internal" href="Lesson2b_Intro_to_TF2_Keras_TFDS_RadiantEarth.html">TensorFlow 2 and Keras quickstart for geospatial computer vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="Lesson3_Intro_tensors_functions_datasets.html">Introduction to Tensors, TensorFlow Functions and TensorFlow Datasets</a></li>

<li class="toctree-l1"><a class="reference internal" href="Lesson4b_Integrations_with_Google_Cloud_Platform.html">What are cloud providers? Why might I need them for ML and Earth observation data?</a></li>



<li class="toctree-l1"><a class="reference internal" href="Lesson5a_prep_data_ML_segmentation.html">Process dataset for use with deep learning segmentation network</a></li>
<li class="toctree-l1"><a class="reference internal" href="Lesson5b_deeplearning_crop_segmentation.html">Semantic segmentation with deep learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Lesson6a_evaluation.html">Evaluating Semantic Segmentation Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="Lesson6b_dealing_with_limited_data.html">Dealing with limited data for semantic segmentation</a></li>

<li class="toctree-l1"><a class="reference internal" href="appendix.html">Appendix</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/developmentseed/tensorflow-eo-training/main?urlpath=tree/ds_book/docs/Lesson1a_Intro_ML_NN_DL.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/developmentseed/tensorflow-eo-training/blob/main/ds_book/docs/Lesson1a_Intro_ML_NN_DL.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/developmentseed/tensorflow-eo-training" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/developmentseed/tensorflow-eo-training/edit/main/ds_book/docs/Lesson1a_Intro_ML_NN_DL.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/developmentseed/tensorflow-eo-training/issues/new?title=Issue%20on%20page%20%2Fdocs/Lesson1a_Intro_ML_NN_DL.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/docs/Lesson1a_Intro_ML_NN_DL.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Introduction to machine learning, neural networks and deep learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Introduction to machine learning, neural networks and deep learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#objectives">Objectives</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-machine-learning">What is Machine Learning?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-deep-learning">What is Deep Learning?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-neural-networks">What are Neural Networks?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-testing-data">Training and Testing Data</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-and-backward-propagation-hyper-parameters-and-learnable-parameters">Forward and backward propagation, hyper-parameters, and learnable parameters</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-machine-learning-for-computer-vision">Types of Machine Learning for Computer Vision</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-convolutional-neural-networks">What are Convolutional Neural Networks?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-kernel-filter">What is a kernel/filter?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-stride">What is stride?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-convolution-operation">What is a convolution operation?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convolution-operation-using-3d-filter">Convolution operation using 3D filter</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-padding">What is padding?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#semantic-segmentation">Semantic Segmentation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#u-net-segmentation-architecture">U-Net Segmentation Architecture</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#transformers-and-self-attention">Transformers and Self Attention</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="introduction-to-machine-learning-neural-networks-and-deep-learning">
<h1>Introduction to machine learning, neural networks and deep learning<a class="headerlink" href="#introduction-to-machine-learning-neural-networks-and-deep-learning" title="Permalink to this heading">#</a></h1>
<section id="objectives">
<h2>Objectives<a class="headerlink" href="#objectives" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Understand the fundamental goals of machine learning and a bit of the field’s history</p></li>
<li><p>Gain familiarity with the mechanics of a neural network, convolutional neural networks, the U-Net architecture, and Transformers</p></li>
<li><p>Discuss considerations for choosing a deep learning architecture for a particular problem</p></li>
</ul>
<p>Below is a video recording of the oral lecture associated with this lesson. It was given by Lilly Thomas, ML Engineer at Development Seed.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">YouTubeVideo</span>

<span class="k">def</span> <span class="nf">display_yotube_video</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">id_</span> <span class="o">=</span> <span class="n">url</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">YouTubeVideo</span><span class="p">(</span><span class="n">id_</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">display_yotube_video</span><span class="p">(</span><span class="s2">&quot;https://www.youtube.com/watch?v=-C3niPVd-zU&quot;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">600</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="800"
            height="600"
            src="https://www.youtube.com/embed/-C3niPVd-zU"
            frameborder="0"
            allowfullscreen
        ></iframe>
        </div></div>
</div>
<section id="what-is-machine-learning">
<h3>What is Machine Learning?<a class="headerlink" href="#what-is-machine-learning" title="Permalink to this heading">#</a></h3>
<p>Machine learning (ML) is a subset of artificial intelligence (AI), which in broad terms, is defined as the ability of a machine to simulate intelligent human behavior.</p>
<figure class="align-default" id="ai-ml-dl-fig">
<a class="reference internal image-reference" href="https://human-centered.ai/wordpress/wp-content/uploads/2017/11/Deep-Learning-subset-of-Machine-Learning-subset-of-Artificial-Intelligence.jpg"><img alt="https://human-centered.ai/wordpress/wp-content/uploads/2017/11/Deep-Learning-subset-of-Machine-Learning-subset-of-Artificial-Intelligence.jpg" src="https://human-centered.ai/wordpress/wp-content/uploads/2017/11/Deep-Learning-subset-of-Machine-Learning-subset-of-Artificial-Intelligence.jpg" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">Figure from <a class="reference external" href="https://human-centered.ai">https://human-centered.ai</a></span><a class="headerlink" href="#ai-ml-dl-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Compared to traditional programming, ML offers:</p>
<ol class="arabic simple">
<li><p>time savings on behalf of the human programmer,</p></li>
<li><p>time savings on behalf of a human manual interpreter,</p></li>
<li><p>reduction of human error,</p></li>
<li><p>scalable decision making</p></li>
</ol>
<p>Humans still have many roles in this process:</p>
<ol class="arabic simple">
<li><p>curating high quality datasets for the machine learning model(s)</p></li>
<li><p>developing or choosing between algorithms and parameters for model development</p></li>
<li><p>evaluating, comparing, and selecting the best model</p></li>
</ol>
<p>There are several subcategories of machine learning, but for monitoring services in remote sensing, we are typically concerned with two types:</p>
<ol class="arabic simple">
<li><p><strong>Supervised machine learning</strong> involves training a model with labeled data sets that explicitly give examples of predictive features and their target attribute(s).</p></li>
<li><p><strong>Unsupervised machine learning</strong> involves tasking a model to search for patterns in data without the guidance of labels.</p></li>
</ol>
</section>
<section id="what-is-deep-learning">
<h3>What is Deep Learning?<a class="headerlink" href="#what-is-deep-learning" title="Permalink to this heading">#</a></h3>
<p>Deep learning is defined by neural networks with depth, i.e. many layers and connections. Deep stacking of many layers with multiple processing units called “neurons” allows deep learning models to learn complex, non-linear features. The deepest layers of a network once trained can infer highly abstract concepts, such as what differentiates a farm from a forest in satellite imagery.</p>
<div class="admonition-cost-of-deep-learning admonition">
<p class="admonition-title"><strong>Cost of deep learning</strong></p>
<p>Deep learning requires a lot of data to learn and usually a significant amount of computing power. Developing datasets and training deep learning models can be expensive depending on the scope of the problem.</p>
</div>
</section>
<section id="what-are-neural-networks">
<h3>What are Neural Networks?<a class="headerlink" href="#what-are-neural-networks" title="Permalink to this heading">#</a></h3>
<p>Artificial neural networks (ANNs) are a specific, biologically-inspired class of machine learning algorithms. They are modeled after the structure and function of the human brain.</p>
<figure class="align-default" id="neuron-fig">
<a class="reference internal image-reference" href="https://github.com/developmentseed/tensorflow-eo-training/blob/main/ds_book/docs/images/neuron-structure.jpg?raw=1"><img alt="https://github.com/developmentseed/tensorflow-eo-training/blob/main/ds_book/docs/images/neuron-structure.jpg?raw=1" src="https://github.com/developmentseed/tensorflow-eo-training/blob/main/ds_book/docs/images/neuron-structure.jpg?raw=1" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">Biological neuron (from <a class="reference external" href="https://training.seer.cancer.gov/anatomy/nervous/tissue.html">https://training.seer.cancer.gov/anatomy/nervous/tissue.html</a>).</span><a class="headerlink" href="#neuron-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>ANNs are essentially programs that make decisions by weighing the evidence and responding to feedback. By varying the input data, types of parameters and their values, we can get different models of decision-making.</p>
<figure class="align-default" id="neuralnet-basic-fig">
<a class="reference internal image-reference" href="https://miro.medium.com/max/1100/1*x6KWjKTOBhUYL0MRX4M3oQ.png"><img alt="https://miro.medium.com/max/1100/1*x6KWjKTOBhUYL0MRX4M3oQ.png" src="https://miro.medium.com/max/1100/1*x6KWjKTOBhUYL0MRX4M3oQ.png" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 3 </span><span class="caption-text">Basic neural network from <a class="reference external" href="https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9">https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9</a>.</span><a class="headerlink" href="#neuralnet-basic-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>In network architectures, neurons are grouped in layers, with synapses traversing the interstitial space between neurons in one layer and the next.</p>
<section id="training-and-testing-data">
<h4>Training and Testing Data<a class="headerlink" href="#training-and-testing-data" title="Permalink to this heading">#</a></h4>
<p>The dataset (e.g. all images and their labels) are split into training, validation and testing sets. A common ratio is 70:20:10 percent, train:validation:test.  If randomly split, it is important to check that all class labels exist in all sets and are well represented.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Why do we need validation and test data? Are they redundant?
We need separate test data to evaluate the performance of the model because the validation data is used during training to measure error and therefore inform updates to the model parameters. Therefore, validation data is not unbiased to the model. A need for new, wholly unseen data to test with is required.</p>
</div>
</section>
<section id="forward-and-backward-propagation-hyper-parameters-and-learnable-parameters">
<h4>Forward and backward propagation, hyper-parameters, and learnable parameters<a class="headerlink" href="#forward-and-backward-propagation-hyper-parameters-and-learnable-parameters" title="Permalink to this heading">#</a></h4>
<p>Neural networks train in cycles, where the input data passes through the network, a relationship between input data and target values is learned, a prediction is made, the prediction value is measured for error relative to its true value, and the errors are used to inform updates to parameters in the network, feeding into the next cycle of learning and prediction using the updated information. This happens through a two-step process called forward propagation and back propagation, in which the first part is used to gather knowledge and the second part is used to correct errors in the model’s knowledge.</p>
<figure class="align-default" id="forward-backprop-fig">
<a class="reference internal image-reference" href="https://thumbs.gfycat.com/BitesizedWeeBlacklemur-max-1mb.gif"><img alt="https://thumbs.gfycat.com/BitesizedWeeBlacklemur-max-1mb.gif" src="https://thumbs.gfycat.com/BitesizedWeeBlacklemur-max-1mb.gif" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4 </span><span class="caption-text"><a class="reference external" href="https://gfycat.com/gifs/search/backpropagation">Forward and back propagation</a>.</span><a class="headerlink" href="#forward-backprop-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The <strong>activation function</strong> decides whether or not the output from one neuron is useful or not based on a threshold value, and therefore, whether it will be carried from one layer to the next.</p>
<p><strong>Weights</strong> control the signal (or the strength of the connection) between two neurons in two consecutive layers.</p>
<p><strong>Biases</strong> are values which help determine whether or not the activation output from a neuron is going to be passed forward through the network.</p>
<p>In a neural network, neurons in one layer are connected to neurons in the next layer. As information passes from one neuron to the next, the information is conditioned by the weight of the synapse and is subjected to a bias. The weights and biases determine if the information passes further beyond the current neuron.</p>
<figure class="align-default" id="activation-fig">
<a class="reference internal image-reference" href="https://cdn-images-1.medium.com/max/651/1*UA30b0mJUPYoPvN8yJr2iQ.jpeg"><img alt="https://cdn-images-1.medium.com/max/651/1*UA30b0mJUPYoPvN8yJr2iQ.jpeg" src="https://cdn-images-1.medium.com/max/651/1*UA30b0mJUPYoPvN8yJr2iQ.jpeg" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5 </span><span class="caption-text"><a class="reference external" href="https://laptrinhx.com/statistics-is-freaking-hard-wtf-is-activation-function-207913705/">Weights, bias, activation</a>.</span><a class="headerlink" href="#activation-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>During training, the weights and biases are learned and updated using the training and validation dataset to fit the data and reduce error of prediction values relative to target values.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<ul class="simple">
<li><p><strong>Activation function</strong>: decides whether or not the output from one neuron is useful or not</p></li>
<li><p><strong>Weights</strong>: control the signal between neurons in consecutive layers</p></li>
<li><p><strong>Biases</strong>: a threshold value that determines the activation of each neuron</p></li>
<li><p>Weights and biases are the learnable parameters of a deep learning model</p></li>
</ul>
</div>
<p>The <strong>learning rate</strong> controls how much we want the model to change in response to the estimated error after each training cycle</p>
<figure class="align-default" id="loss-curve-fig">
<a class="reference internal image-reference" href="https://d1zx6djv3kb1v7.cloudfront.net/wp-content/media/2019/09/Neural-network-32-i2tutorials.png"><img alt="https://d1zx6djv3kb1v7.cloudfront.net/wp-content/media/2019/09/Neural-network-32-i2tutorials.png" src="https://d1zx6djv3kb1v7.cloudfront.net/wp-content/media/2019/09/Neural-network-32-i2tutorials.png" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 6 </span><span class="caption-text"><a class="reference external" href="https://www.i2tutorials.com/what-are-local-minima-and-global-minima-in-gradient-descent/">Local vs. global minimum (the optimal point to reach)</a>.</span><a class="headerlink" href="#loss-curve-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The <strong>batch size</strong> determines the portion of our training dataset that can be fed to the model during each cycle. Stated otherwise, batch size controls the number of training samples to work through before the model’s internal parameters are updated.</p>
<figure class="align-default" id="batch-epoch-fig">
<a class="reference internal image-reference" href="https://www.baeldung.com/wp-content/uploads/sites/4/2020/12/epoch-batch-size.png"><img alt="https://www.baeldung.com/wp-content/uploads/sites/4/2020/12/epoch-batch-size.png" src="https://www.baeldung.com/wp-content/uploads/sites/4/2020/12/epoch-batch-size.png" style="width: 250px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 7 </span><span class="caption-text"><a class="reference external" href="https://www.baeldung.com/cs/epoch-neural-networks">Modulating batch size detetmines how many iterations are within one epoch</a>.</span><a class="headerlink" href="#batch-epoch-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>An <strong>epoch</strong> is defined as the point when all training samples, aka the entire dataset, has passed through the neural network once. The number of epochs controls how many times the entire dataset is cycled through and analyzed by the neural network. Related, but not necessarily as a parameter is an <strong>iteration</strong>, which is the pass of one batch through the network. If the batch size is smaller than the size of the whole dataset, then there are multiple iterations in one epoch.</p>
<p>The <strong>optimization function</strong> is really important. It’s what we use to change the attributes of your neural network such as weights and biases in order to reduce the losses. The goal of an optimization function is to minimize the error produced by the model.</p>
<p>The <strong>loss function</strong>, also known as the cost function, measures how much the model needs to improve based on the prediction errors relative to the true values during training.</p>
<figure class="align-default" id="id1">
<a class="reference internal image-reference" href="https://miro.medium.com/max/810/1*UUHvSixG7rX2EfNFTtqBDA.gif"><img alt="https://miro.medium.com/max/810/1*UUHvSixG7rX2EfNFTtqBDA.gif" src="https://miro.medium.com/max/810/1*UUHvSixG7rX2EfNFTtqBDA.gif" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8 </span><span class="caption-text"><a class="reference external" href="https://towardsdatascience.com/machine-learning-fundamentals-via-linear-regression-41a5d11f5220">Loss curve</a>.</span><a class="headerlink" href="#id1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The <strong>accuracy metric</strong> measures the performance of a model. For example, a pixel to pixel comparison for agreement on class.</p>
<p>Note: the <strong>activation function</strong> is also a hyper-parameter.</p>
</section>
<section id="types-of-machine-learning-for-computer-vision">
<h4>Types of Machine Learning for Computer Vision<a class="headerlink" href="#types-of-machine-learning-for-computer-vision" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Image classification: classifying whole images, e.g. image with clouds, image without clouds</p></li>
<li><p>Object detection: identifying locations of objects in an image and classifying them, e.g. identify bounding boxes of cars and planes in satellite imagery</p></li>
<li><p>Semantic segmentation: classifying individual pixels in an image, e.g. land cover classification</p></li>
<li><p>Instance segmentation: classifying individual pixels in an image in terms of both class and individual membership, e.g. detecting unique agricultural field polygons and classifying them</p></li>
<li><p>Generative Adversarial:  a type of image generation where synthetic images are created from real ones, e.g. creating synthetic landscapes from real landscape images</p></li>
</ul>
<p>For all computer vision tasks above, CNNs have played a large role in the advancement of the field. Let’s dive into what CNNs are and how they learn.</p>
</section>
<section id="what-are-convolutional-neural-networks">
<h4>What are Convolutional Neural Networks?<a class="headerlink" href="#what-are-convolutional-neural-networks" title="Permalink to this heading">#</a></h4>
<p>A Convolutional Neural Network (ConvNet/CNN) is a deep learning architecture inspired by the organization of the human visual cortex, in which individual neurons respond to stimuli within a constrained region of the visual field known as the receptive field. Several receptive fields overlap to account for the entire visual area.</p>
<p>In artificial CNNs, an input matrix such as an image is given importance per various aspects and objects in the image through a moving, convolving receptive field. Very little pre-processing is required for CNNs relative to other classification methods as the need for upfront feature-engineering is removed. Rather, CNNs learn features on their own, provided enough training time and informative examples.</p>
<figure class="align-default" id="convolution-fig">
<a class="reference internal image-reference" href="https://miro.medium.com/max/1400/1*Fw-ehcNBR9byHtho-Rxbtw.gif"><img alt="https://miro.medium.com/max/1400/1*Fw-ehcNBR9byHtho-Rxbtw.gif" src="https://miro.medium.com/max/1400/1*Fw-ehcNBR9byHtho-Rxbtw.gif" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 9 </span><span class="caption-text">Convolution of a kernal over an input matrix from <a class="reference external" href="https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1">https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1</a>.</span><a class="headerlink" href="#convolution-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="what-is-a-kernel-filter">
<h4>What is a kernel/filter?<a class="headerlink" href="#what-is-a-kernel-filter" title="Permalink to this heading">#</a></h4>
<p>A kernel is a matrix smaller than the input. It acts as a receptive field that moves over the input matrix from left to right and top to bottom and filters for features in the image. Depending on the contents of the kernel, it may activate when passed over different edges or shapes.</p>
</section>
<section id="what-is-stride">
<h4>What is stride?<a class="headerlink" href="#what-is-stride" title="Permalink to this heading">#</a></h4>
<p>Stride refers to the number of pixels that the kernel shifts at each step in its navigation of the input matrix.</p>
</section>
<section id="what-is-a-convolution-operation">
<h4>What is a convolution operation?<a class="headerlink" href="#what-is-a-convolution-operation" title="Permalink to this heading">#</a></h4>
<p>The convolution operation is the combination of two functions to produce a third function as a result. It merges two sets of information, the kernel and the input matrix.</p>
<figure class="align-default" id="convolution-arithmetic-fig">
<a class="reference internal image-reference" href="https://theano-pymc.readthedocs.io/en/latest/_images/numerical_no_padding_no_strides.gif"><img alt="https://theano-pymc.readthedocs.io/en/latest/_images/numerical_no_padding_no_strides.gif" src="https://theano-pymc.readthedocs.io/en/latest/_images/numerical_no_padding_no_strides.gif" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10 </span><span class="caption-text">Convolution of a kernel over an input matrix from <a class="reference external" href="https://theano-pymc.readthedocs.io/en/latest/tutorial/conv_arithmetic.html">https://theano-pymc.readthedocs.io/en/latest/tutorial/conv_arithmetic.html</a>.</span><a class="headerlink" href="#convolution-arithmetic-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="convolution-operation-using-3d-filter">
<h4>Convolution operation using 3D filter<a class="headerlink" href="#convolution-operation-using-3d-filter" title="Permalink to this heading">#</a></h4>
<p>An input image is often represented as a 3D matrix with a dimension for width (pixels), height (pixels), and depth (channels). In the case of an optical image with red, green and blue channels, the kernel/filter matrix is shaped with the same channel depth as the input and the weighted sum of dot products is computed across all 3 dimensions.</p>
</section>
<section id="what-is-padding">
<h4>What is padding?<a class="headerlink" href="#what-is-padding" title="Permalink to this heading">#</a></h4>
<p>After a convolution operation, the feature map is by default smaller than the original input matrix.</p>
<figure class="align-default" id="multi-layer-cnn-fig">
<a class="reference internal image-reference" href="https://www.researchgate.net/profile/Sheraz-Khan-14/publication/321586653/figure/fig4/AS:568546847014912&#64;1512563539828/The-LeNet-5-Architecture-a-convolutional-neural-network.png"><img alt="https://www.researchgate.net/profile/Sheraz-Khan-14/publication/321586653/figure/fig4/AS:568546847014912&#64;1512563539828/The-LeNet-5-Architecture-a-convolutional-neural-network.png" src="https://www.researchgate.net/profile/Sheraz-Khan-14/publication/321586653/figure/fig4/AS:568546847014912&#64;1512563539828/The-LeNet-5-Architecture-a-convolutional-neural-network.png" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11 </span><span class="caption-text"><a class="reference external" href="https://www.researchgate.net/figure/The-LeNet-5-Architecture-a-convolutional-neural-network_fig4_321586653">Progressive downsizing of feature maps in a multi-layer CNN</a>.</span><a class="headerlink" href="#multi-layer-cnn-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>To maintain the same spatial dimensions between input matrix and output feature map, we may pad the input matrix with a border of zeroes or ones. There are two types of padding:</p>
<ol class="arabic simple">
<li><p>Same padding: a border of zeroes or ones is added to match the input/output dimensions</p></li>
<li><p>Valid padding: no border is added and the output dimensions are not matched to the input</p></li>
</ol>
<figure class="align-default" id="padding-fig">
<a class="reference internal image-reference" href="https://miro.medium.com/max/666/1*noYcUAa_P8nRilg3Lt_nuA.png"><img alt="https://miro.medium.com/max/666/1*noYcUAa_P8nRilg3Lt_nuA.png" src="https://miro.medium.com/max/666/1*noYcUAa_P8nRilg3Lt_nuA.png" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12 </span><span class="caption-text"><a class="reference external" href="https://ayeshmanthaperera.medium.com/what-is-padding-in-cnns-71b21fb0dd7">Padding an input matrix with zeroes</a>.</span><a class="headerlink" href="#padding-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="semantic-segmentation">
<h4>Semantic Segmentation<a class="headerlink" href="#semantic-segmentation" title="Permalink to this heading">#</a></h4>
<p>To pair with the content of these tutorials, we will demonstrate semantic segmentation (supervised) to map land use categories and illegal gold mining activity.</p>
<ul class="simple">
<li><p>Semantic = of or relating to meaning (class)</p></li>
<li><p>Segmentation = division (of image) into separate parts</p></li>
</ul>
</section>
<section id="u-net-segmentation-architecture">
<h4>U-Net Segmentation Architecture<a class="headerlink" href="#u-net-segmentation-architecture" title="Permalink to this heading">#</a></h4>
<p>Semantic segmentation is often distilled into the combination of an encoder and a decoder. An encoder generates logic or feedback from input data, and a decoder takes that feedback and translates it to output data in the same form as the input.</p>
<p>The U-Net model, which is one of many deep learning segmentation algorithms, has a great illustration of this structure.</p>
<figure class="align-default" id="unet-fig">
<a class="reference internal image-reference" href="https://developers.arcgis.com/assets/img/python-graphics/unet.png"><img alt="https://developers.arcgis.com/assets/img/python-graphics/unet.png" src="https://developers.arcgis.com/assets/img/python-graphics/unet.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 13 </span><span class="caption-text">U-Net architecture (from <a class="reference external" href="https://arxiv.org/abs/1505.04597">Ronneberger et al., 2015</a>).</span><a class="headerlink" href="#unet-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>In Fig. 13, the encoder is on the left side of the model. It consists of consecutive convolutional layers, each followed by ReLU and a max pooling operation to encode feature representations at multiple scales. The encoder can be represented by most feature extraction networks designed for classification.</p>
<p>The decoder, on the right side of the Fig. 13 diagram, is tasked to semantically project the discriminative features learned by the encoder onto the original pixel space to render a dense classification. The decoder consists of deconvolution and concatenation followed by regular convolution operations.</p>
<p>Following the decoder is the final classification layer, which computes the pixel-wise classification for each cell in the final feature map.</p>
<p>ReLU is an operation, an activation function to be specific, that induces non-linearity. This function intakes the feature map from a convolution operation and remaps it such that any positive value stays exactly the same, and any negative value becomes zero.</p>
<figure class="align-default" id="relu-graph-fig">
<a class="reference internal image-reference" href="https://miro.medium.com/max/3200/1*w48zY6o9_5W9iesSsNabmQ.gif"><img alt="https://miro.medium.com/max/3200/1*w48zY6o9_5W9iesSsNabmQ.gif" src="https://miro.medium.com/max/3200/1*w48zY6o9_5W9iesSsNabmQ.gif" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 14 </span><span class="caption-text"><a class="reference external" href="https://medium.com/ai%C2%B3-theory-practice-business/magic-behind-activation-function-c6fbc5e36a92">ReLU activation function</a>.</span><a class="headerlink" href="#relu-graph-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="relu-maxpooling-fig">
<a class="reference internal image-reference" href="https://miro.medium.com/max/1000/1*cmGESKfSZLH2ksqF_kBgfQ.gif"><img alt="https://miro.medium.com/max/1000/1*cmGESKfSZLH2ksqF_kBgfQ.gif" src="https://miro.medium.com/max/1000/1*cmGESKfSZLH2ksqF_kBgfQ.gif" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 15 </span><span class="caption-text"><a class="reference external" href="https://towardsdatascience.com/a-laymans-guide-to-building-your-first-image-classification-model-in-r-using-keras-b285deac6572">ReLU applied to an input matrix</a>.</span><a class="headerlink" href="#relu-maxpooling-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Max pooling is used to summarize a feature map and only retain the important structural elements, foregoing the more granular detail that may not be significant to the modeling task. This helps to denoise the signal and helps with computational efficiency. It works similar to convolution in that a kernel with a stride is applied to the feature map and only the maximum value within each patch is reserved.</p>
<figure class="align-default" id="maxpooling-fig">
<a class="reference internal image-reference" href="https://thumbs.gfycat.com/FirstMediumDalmatian-size_restricted.gif"><img alt="https://thumbs.gfycat.com/FirstMediumDalmatian-size_restricted.gif" src="https://thumbs.gfycat.com/FirstMediumDalmatian-size_restricted.gif" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 16 </span><span class="caption-text"><a class="reference external" href="https://gfycat.com/firstmediumdalmatian">Max pooling with a kernal over an input matrix</a>.</span><a class="headerlink" href="#maxpooling-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="transformers-and-self-attention">
<h1>Transformers and Self Attention<a class="headerlink" href="#transformers-and-self-attention" title="Permalink to this heading">#</a></h1>
<p>The Transformer model was introduced in the paper “Attention is all you Need”. It motivates the Transformer and self attention for a few reasons:</p>
<ul class="simple">
<li><p>For modeling with sequences, like pixel time series, or lengths of text, RNNs are hard to parallelize and difficult to train.</p></li>
<li><p>CNN based methods increase the number of computations needed to relate two input positions. So it is harder for a CNN based sequence model to capture long term memory effects</p></li>
<li><p>In contrast, multi-head self attention and the transformer architecture are easier to train, easy to parallelize, and input values can be related to each other in constant time no matter how far apart they are.</p></li>
</ul>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="index.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Deep Learning with TensorFlow: <br> Tutorials for modeling LULC.</p>
      </div>
    </a>
    <a class="right-next"
       href="Lesson2a_Intro_to_Google_Colab.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Introduction to Google Colab</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Introduction to machine learning, neural networks and deep learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#objectives">Objectives</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-machine-learning">What is Machine Learning?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-deep-learning">What is Deep Learning?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-neural-networks">What are Neural Networks?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-testing-data">Training and Testing Data</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-and-backward-propagation-hyper-parameters-and-learnable-parameters">Forward and backward propagation, hyper-parameters, and learnable parameters</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-machine-learning-for-computer-vision">Types of Machine Learning for Computer Vision</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-convolutional-neural-networks">What are Convolutional Neural Networks?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-kernel-filter">What is a kernel/filter?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-stride">What is stride?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-convolution-operation">What is a convolution operation?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#convolution-operation-using-3d-filter">Convolution operation using 3D filter</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-padding">What is padding?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#semantic-segmentation">Semantic Segmentation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#u-net-segmentation-architecture">U-Net Segmentation Architecture</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#transformers-and-self-attention">Transformers and Self Attention</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Development Seed
</p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>