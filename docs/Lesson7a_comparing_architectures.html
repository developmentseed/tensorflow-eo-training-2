
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Comparing deep learning architectures for different computer vision tasks on satellite imagery &#8212; Deep learning with TensorFlow</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/Lesson7a_comparing_architectures';</script>
    <link rel="icon" href="../_static/ds.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Comparing RNNs and Transformers for Imagery" href="Lesson7b_comparing_RNN_transformer_architectures.html" />
    <link rel="prev" title="Dealing with limited data for semantic segmentation" href="Lesson6b_dealing_with_limited_data.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/ds.png" class="logo__image only-light" alt="Deep learning with TensorFlow - Home"/>
    <script>document.write(`<img src="../_static/ds.png" class="logo__image only-dark" alt="Deep learning with TensorFlow - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Lesson1a_Intro_ML_NN_DL.html">Introduction to machine learning, neural networks and deep learning</a></li>

<li class="toctree-l1"><a class="reference internal" href="Lesson2a_Intro_to_Google_Colab.html">Introduction to Google Colab</a></li>



<li class="toctree-l1"><a class="reference internal" href="Lesson2b_Intro_TensorFlow_Keras.html">Introduction to TensorFlow 2 and Keras</a></li>
<li class="toctree-l1"><a class="reference internal" href="Lesson2c_Intro_TF2_Keras_TFDS_RadiantEarth.html">TensorFlow 2 and Keras quickstart for geospatial computer vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="Lesson3_Intro_tensors_functions_datasets.html">Introduction to Tensors, TensorFlow Functions and TensorFlow Datasets</a></li>


<li class="toctree-l1"><a class="reference internal" href="Lesson4a_GEE_PythonAPI_TensorFlow_Regression.html">Regression using TensorFlow with Google Earth Engine Python API</a></li>








<li class="toctree-l1"><a class="reference internal" href="Lesson4b_Integrations_with_Google_Cloud_Platform.html">What are cloud providers? Why might I need them for ML and Earth observation data?</a></li>




<li class="toctree-l1"><a class="reference internal" href="Lesson5a_prep_data_ML_segmentation.html">Processing earth observation data for semantic segmentation with deep learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Lesson5b_deeplearning_segmentation_UNet.html">Semantic segmentation with Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="Lesson5c_segmentation_ViT.html">Semantic segmentation with Vision Transformers, Hugging Face and TensorFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="Lesson6a_evaluation.html">Evaluating Semantic Segmentation Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="Lesson6b_dealing_with_limited_data.html">Dealing with limited data for semantic segmentation</a></li>

<li class="toctree-l1 current active"><a class="current reference internal" href="#">Comparing deep learning architectures for different computer vision tasks on satellite imagery</a></li>
<li class="toctree-l1"><a class="reference internal" href="Lesson7b_comparing_RNN_transformer_architectures.html">Comparing RNNs and Transformers for Imagery</a></li>
<li class="toctree-l1"><a class="reference internal" href="Lesson7c_transfer_learning_hyperparam_opt.html">Transfer learning, fine-tuning and hyperparameter tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Lesson8_advanced_applications.html">TensorFlow advanced applications with deep learning object detection, time-series analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix.html">Appendix</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/developmentseed/tensorflow-eo-training-2/main?urlpath=tree/ds_book/docs/Lesson7a_comparing_architectures.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/developmentseed/tensorflow-eo-training-2/blob/main/ds_book/docs/Lesson7a_comparing_architectures.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/developmentseed/tensorflow-eo-training-2" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/developmentseed/tensorflow-eo-training-2/edit/main/ds_book/docs/Lesson7a_comparing_architectures.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/developmentseed/tensorflow-eo-training-2/issues/new?title=Issue%20on%20page%20%2Fdocs/Lesson7a_comparing_architectures.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/docs/Lesson7a_comparing_architectures.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Comparing deep learning architectures for different computer vision tasks on satellite imagery</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lenet-lecun-et-al-1998-i">LeNet (LeCun et al. 1998)[i]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#alexnet-krizhevsky-et-al-2012-ii">AlexNet  (Krizhevsky et al. 2012)[ii]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#network-in-network-nin-lin-et-al-2013-iii">Network in Network (NiN) (Lin et al. 2013)[iii]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#networks-using-blocks-vgg-simonyan-and-zisserman-2014-iv">Networks Using Blocks, VGG (Simonyan and Zisserman 2014)[iv]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inception-googlenet-szegedy-et-al-2014-v">Inception/GoogleNet (Szegedy et al. 2014)[v]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-normalization-ioffe-and-szegedy-2015-vi">Batch Normalization (Ioffe and Szegedy 2015)[vi]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resnet-he-et-al-2015-vii">Resnet (He et al. 2015)[vii]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#concluding-remarks">Concluding Remarks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References:</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="comparing-deep-learning-architectures-for-different-computer-vision-tasks-on-satellite-imagery">
<h1>Comparing deep learning architectures for different computer vision tasks on satellite imagery<a class="headerlink" href="#comparing-deep-learning-architectures-for-different-computer-vision-tasks-on-satellite-imagery" title="Link to this heading">#</a></h1>
<blockquote>
<div><p>A guide for selecting deep-learning architectures for land use / land cover mapping and change detection in satellite imagery.</p>
</div></blockquote>
<section id="lenet-lecun-et-al-1998-i">
<h2>LeNet (LeCun et al. 1998)[i]<a class="headerlink" href="#lenet-lecun-et-al-1998-i" title="Link to this heading">#</a></h2>
<p>In 1998, LeCun et al. introduced the first Convolutional Neural Network (CNN) known as LeNet. This marked the introduction of the convolutional layer which is a three-part process involving convolution, pooling, and nonlinear activation functions. The LeNet model utilized the tanh activation function, though later networks would replace this with the Rectified Linear Unit (ReLU) to address the vanishing gradient problem. It also uses average pooling, though later research found max pooling performs better. Despite its innovative design, LeNet was not popular during its inception, mainly due to the lack of software and hardware compatibility for training large networks. At that time, Support Vector Machines (SVM) were still competitive with LeNet in terms of detection capabilities.</p>
<figure class="align-default" id="lenetfig">
<a class="reference internal image-reference" href="https://d2l.ai/_images/lenet.svg"><img alt="https://d2l.ai/_images/lenet.svg" src="https://d2l.ai/_images/lenet.svg" width="450px" /></a>
<figcaption>
<p><span class="caption-number">Fig. 29 </span><span class="caption-text"><a class="reference external" href="https://d2l.ai/_images/lenet.svg">LeNET Image</a> from “Dive into Deep Learning” by <a class="reference external" href="http://d2l.ai">d2l.ai</a>, used under CC BY-SA 4.0</span><a class="headerlink" href="#lenetfig" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="alexnet-krizhevsky-et-al-2012-ii">
<h2>AlexNet  (Krizhevsky et al. 2012)[ii]<a class="headerlink" href="#alexnet-krizhevsky-et-al-2012-ii" title="Link to this heading">#</a></h2>
<p>2012 marked the breakout moment for CNNs with the introduction of AlexNet. It significantly outperformed its competition, achieving an error rate that was 10.8 percentage points lower than the next runner-up in the ImageNet Large Scale Visual Recognition Challenge. One of the primary takeaways from AlexNet’s performance was that deeper networks tended to yield higher performance. The computational demands of AlexNet were made feasible primarily because of GPUs. The network improved upon LeNeT by utilizing the non-saturating ReLU function, which showed better training performance than the tanh or sigmoid activations, max pooling instead of average pooling, and a regularization layer called dropout, which reduces overfitting by Randomly setting nodes to 0 so that they do not contribute to training. Finally, the largest difference between AlexNet and LeNet is that AlexNet was trained on a massive amount of data, millions of images, which was enabled by a combination of GPU hardware, and the aforementioned changes to the architecture: ReLU for faster training and dropout for improved regularization. AlexNet’s impact is undeniable, as evidenced by its citation in over 120,000 papers on Google Scholar.</p>
<figure class="align-default" id="alexnetarchfig">
<a class="reference internal image-reference" href="https://learnopencv.com/wp-content/uploads/2018/05/AlexNet-1.png"><img alt="https://learnopencv.com/wp-content/uploads/2018/05/AlexNet-1.png" src="https://learnopencv.com/wp-content/uploads/2018/05/AlexNet-1.png" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 30 </span><span class="caption-text"><a class="reference external" href="https://learnopencv.com/wp-content/uploads/2018/05/AlexNet-1.png">AlexNet Arch Image</a> from “Understanding AlexNet” by <a class="reference external" href="http://learnopencv.com">learnopencv.com</a></span><a class="headerlink" href="#alexnetarchfig" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Below are the features learned by the initial layer of AlexNet. Note how some convolutional kernels learn textural information, like lines, and others learn color.</p>
<figure class="align-default" id="alexnetfilterfig">
<a class="reference internal image-reference" href="https://d2l.ai/_images/filters.png"><img alt="https://d2l.ai/_images/filters.png" src="https://d2l.ai/_images/filters.png" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 31 </span><span class="caption-text"><a class="reference external" href="https://d2l.ai/_images/filters.png">AlexNet FilterImage</a> from “Dive into Deep Learning” by <a class="reference external" href="http://d2l.ai">d2l.ai</a>, used under CC BY-SA 4.0</span><a class="headerlink" href="#alexnetfilterfig" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="network-in-network-nin-lin-et-al-2013-iii">
<h2>Network in Network (NiN) (Lin et al. 2013)[iii]<a class="headerlink" href="#network-in-network-nin-lin-et-al-2013-iii" title="Link to this heading">#</a></h2>
<p>This architecture addresses a fundamental problem with AlexNet, LeNet, and other CNNs with fully connected layers: the last fully connected layers have hundreds of millions of parameters. NiN solves this by replacing the last fully connected layers with 1x1 convolutions and global average pooling. It also replaces linear convolutional layers with mini multi-layer-perceptrons appended to convolutional layers.</p>
<figure class="align-default" id="ninfilterfig">
<a class="reference internal image-reference" href="../_images/nin_mlp_conv.png"><img alt="../_images/nin_mlp_conv.png" src="../_images/nin_mlp_conv.png" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 32 </span><span class="caption-text"><a class="reference download internal" download="" href="../_downloads/d7c80a49d75dc93bf0d53f82528d7fb8/nin_mlp_conv.png"><span class="xref download myst">NiN Block</span></a> from “Network in Network” by Lin et al. 2013</span><a class="headerlink" href="#ninfilterfig" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Effectively this creates many smaller fully connected layers at each pixel location that in total have many fewer parameters than the linearly stacked fully connected layers of AlexNet. Notably, NiN can increase the training time for the network to converge to a minimum loss.</p>
<figure class="align-default" id="id1">
<a class="reference internal image-reference" href="../_images/NIN.png"><img alt="../_images/NIN.png" src="../_images/NIN.png" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 33 </span><span class="caption-text"><a class="reference download internal" download="" href="../_downloads/cf6380488594960418846620dd9ce67d/NIN.png"><span class="xref download myst">NiN Architecture</span></a> from “Network in Network” by by Lin et al. 2013</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="networks-using-blocks-vgg-simonyan-and-zisserman-2014-iv">
<h2>Networks Using Blocks, VGG (Simonyan and Zisserman 2014)[iv]<a class="headerlink" href="#networks-using-blocks-vgg-simonyan-and-zisserman-2014-iv" title="Link to this heading">#</a></h2>
<p>While VGG still maintains fully connected layers that NiN addresses, it introduces a solution to a different key limitation of AlexNet: CNN based networks with stacked blocks of Conv &gt; Relu &gt; Max Pooling can only be as deep given the second logarithm of the number of dimensions in the input. The authors introduced the VGG Block, which replaces single convolutional steps with multiple convolutional steps. For example, two stacked 3x3 convolutions followed by max pooling produces a similar computation burden to a single 5x5 convolutional layer. They find that deeper and narrower networks can be trained to higher performance, while accepting higher parameters counts.</p>
<figure class="align-default" id="vggfig">
<a class="reference internal image-reference" href="https://d2l.ai/_images/vgg.svg"><img alt="https://d2l.ai/_images/vgg.svg" src="https://d2l.ai/_images/vgg.svg" width="450px" /></a>
<figcaption>
<p><span class="caption-number">Fig. 34 </span><span class="caption-text"><a class="reference external" href="https://d2l.ai/_images/vgg.svg">VGG Architecture Image</a> from “Dive into Deep Learning” by <a class="reference external" href="http://d2l.ai">d2l.ai</a>, used under CC BY-SA 4.0</span><a class="headerlink" href="#vggfig" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="inception-googlenet-szegedy-et-al-2014-v">
<h2>Inception/GoogleNet (Szegedy et al. 2014)[v]<a class="headerlink" href="#inception-googlenet-szegedy-et-al-2014-v" title="Link to this heading">#</a></h2>
<p>Inception, also known as GoogleNet, combines the best features of VGG and NiN to reduce the parameter size of the model and learn features more efficiently. It does so by using a 3 part structure for a CNN composed of the following:</p>
<ol class="arabic simple">
<li><p>a simple stem network for learning low-level features</p></li>
<li><p>a body for learning higher level features with more intricate network structure</p></li>
<li><p>a head network for a particular kind, or kinds, of detection (classification, segmentation, object detection).</p></li>
</ol>
<p>This 3-part structure reflects many of the modern networks we use today and allowed GoogleNet to learn more informative features with fewer parameters. The main star of GoogleNet, the inception block, occurs in the body, and uses multiple branches to allow the network to implicitly select the correct convolution filter shapes for a particular detection problem. This means parameters are allocated to the most important convolution filters that map to different extents of an image. A noteworthy statement from the authors is, “For most of the experiments, the models were designed to keep a computational budget of 1.5 billion multiply-adds at inference time, so that they do not end up to be a purely academic curiosity, but could be put to real world use, even on large datasets, at a reasonable cost.” (Szegedy et al. 2014).</p>
<figure class="align-default" id="inceptionmultibranchfig">
<a class="reference internal image-reference" href="https://d2l.ai/_images/inception.svg"><img alt="https://d2l.ai/_images/inception.svg" src="https://d2l.ai/_images/inception.svg" width="450px" /></a>
<figcaption>
<p><span class="caption-number">Fig. 35 </span><span class="caption-text"><a class="reference external" href="https://d2l.ai/_images/inception.svg">Inception Block Image</a> from “Dive into Deep Learning” by <a class="reference external" href="http://d2l.ai">d2l.ai</a>, used under CC BY-SA 4.0</span><a class="headerlink" href="#inceptionmultibranchfig" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The Inception model also includes some other notable features. The stem module resembles AlexNet and LeNet, but with added 1x1 convolutional filters after the max pooling layers also addressed concerns that many max pooling layers result in the loss of high-resolution spatial information.</p>
<p>This inception module addresses a prevalent issue with deep neural networks: models with a large number of parameters are more susceptible to overfitting, especially when the dataset size remains fixed. Another challenge tackled by Inception was the computational burden of deep vision networks. When two convolutional filters are chained together, a uniform increase in their filter count results in a quadratic increase in computation. In contrast, inception blocks are more computationally efficient.</p>
<img src="./images/quadratic_computation_burden_cnn.png" width="400" height="300"/>
<p>Inception increased both the depth and width of the network but did so with fewer parameters. Interestingly, it possesses fewer parameters than AlexNet, reflecting a broader trend towards creating smaller yet high-performing deep neural networks. The advantages of having fewer parameters include faster training times, reduced memory consumption, and more efficient learned features.</p>
<figure class="align-default" id="googlenetfig">
<a class="reference internal image-reference" href="https://d2l.ai/_images/inception-full-90.svg"><img alt="https://d2l.ai/_images/inception-full-90.svg" src="https://d2l.ai/_images/inception-full-90.svg" width="450px" /></a>
<figcaption>
<p><span class="caption-number">Fig. 36 </span><span class="caption-text"><a class="reference external" href="https://d2l.ai/_images/inception-full-90.svg">GoogleNet Image</a> from “Dive into Deep Learning” by <a class="reference external" href="http://d2l.ai">d2l.ai</a>, used under CC BY-SA 4.0</span><a class="headerlink" href="#googlenetfig" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="batch-normalization-ioffe-and-szegedy-2015-vi">
<h2>Batch Normalization (Ioffe and Szegedy 2015)[vi]<a class="headerlink" href="#batch-normalization-ioffe-and-szegedy-2015-vi" title="Link to this heading">#</a></h2>
<p>Training neural networks can take a long time, even when using GPUs (hours, days, even weeks!). Larger networks, in the 100s of layers, used to be intractable to train and fine-tune on consumer-grade hardware (1 or 2 GPUs). Recall that while VGG improved network accuracy with deeper and narrower networks, it increases parameter counts. VGG-11 only had 8 convolutional layers and 3 connected layers, but today, it’s common for networks to have over 100 convolutional layers. As deeper networks are trained, weights and biases in one layer may be magnified or minified compared to the values in another layer by orders of magnitude, making it more difficult for intermediate layers in the network to update parameters with fixed learning rates and step sizes. This problem is sometimes referred to as “internal covariate shift” indicating that the relationship between parameters in intermediate layers can drift and introduce inaccuracies.</p>
<p>Batch normalization addresses this by subtracting the mean and dividing by the standard deviation for each minibatch. For convolutional layers, this entails taking the channel-wise means and standard deviations across all pixel locations and using them to normalize each channel. Batch norm can be applied to all convolutional layers, or some convolutional layers. Normalization can also be applied ber sample rather than per-batch, which is called layer normalization.</p>
<p>Batch normalization was shown to improve deep model training via regularization and improving numerical stability during training.</p>
</section>
<section id="resnet-he-et-al-2015-vii">
<h2>Resnet (He et al. 2015)[vii]<a class="headerlink" href="#resnet-he-et-al-2015-vii" title="Link to this heading">#</a></h2>
<p>Following the trend of increasing network complexity, the paper “Deep residual learning for image recognition”, investigated how to increase the expressivity of layers in very deep networks, rather than adding parameters that don’t improve the model (it’s been shown mathematically that larger models aren’t guaranteed to lead to better results than smaller models, all else being equal). He et al. sought to improve upon architecture designs by allowing the input to a block to pass through to be added to the activations of the output of the block. Effectively, this means that a block learns the residuals (the differences) between the input and desired activations.</p>
<figure class="align-default" id="residual-block">
<a class="reference internal image-reference" href="https://d2l.ai/_images/residual-block.svg"><img alt="https://d2l.ai/_images/residual-block.svg" src="https://d2l.ai/_images/residual-block.svg" width="450px" /></a>
<figcaption>
<p><span class="caption-number">Fig. 37 </span><span class="caption-text"><a class="reference external" href="https://d2l.ai/_images/residual-block.svg">GoogleNet Image</a> from “Dive into Deep Learning” by <a class="reference external" href="http://d2l.ai">d2l.ai</a>, used under CC BY-SA 4.0</span><a class="headerlink" href="#residual-block" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Therefore, inputs can pass through the network much more quickly, allowing for networks with hundreds of layers (like Resenet-152). With these large networks, inputs can pass through layers that are not needed, since they effectively can learn weights and biases that contribute 0 to the activation, allowing for faster learning.</p>
<p>Residual connections are now a primary component of non-CNN architectures in domains beyond computer vision. The Transformer architecture, which we will learn more about later, also uses residual connections.</p>
</section>
<section id="concluding-remarks">
<h2>Concluding Remarks<a class="headerlink" href="#concluding-remarks" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Importance of using well-established architectures.</p></li>
<li><p>Importance of using well-established implementations, or understanding your own implementation.</p></li>
<li><p>Value of leveraging popular libraries and ML frameworks for practical applications, see next lesson.</p></li>
</ul>
</section>
<section id="references">
<h2>References:<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>[i] <a class="reference external" href="https://ieeexplore.ieee.org/document/726791">LeCun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P. (1998). Gradient-based learning applied to document recognition.</a></p></li>
<li><p>[ii] <a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks.</a></p></li>
<li><p>[iii] <a class="reference external" href="https://arxiv.org/abs/1312.4400">Lin, M., Chen, Q., &amp; Yan, S. (2013). Network in network.</a></p></li>
<li><p>[iv] <a class="reference external" href="https://arxiv.org/abs/1409.1556">Simonyan, K., &amp; Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition.</a></p></li>
<li><p>[v] <a class="reference external" href="https://arxiv.org/abs/1409.4842">Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., … &amp; Rabinovich, A. (2014). Going deeper with convolutions.</a></p></li>
<li><p>[vi] <a class="reference external" href="https://arxiv.org/abs/1502.03167">Ioffe, S., &amp; Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift.</a></p></li>
<li><p>[vii] <a class="reference external" href="https://arxiv.org/abs/1512.03385">He, K., Zhang, X., Ren, S., &amp; Sun, J. (2015). Deep residual learning for image recognition.</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Lesson6b_dealing_with_limited_data.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Dealing with limited data for semantic segmentation</p>
      </div>
    </a>
    <a class="right-next"
       href="Lesson7b_comparing_RNN_transformer_architectures.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Comparing RNNs and Transformers for Imagery</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lenet-lecun-et-al-1998-i">LeNet (LeCun et al. 1998)[i]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#alexnet-krizhevsky-et-al-2012-ii">AlexNet  (Krizhevsky et al. 2012)[ii]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#network-in-network-nin-lin-et-al-2013-iii">Network in Network (NiN) (Lin et al. 2013)[iii]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#networks-using-blocks-vgg-simonyan-and-zisserman-2014-iv">Networks Using Blocks, VGG (Simonyan and Zisserman 2014)[iv]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inception-googlenet-szegedy-et-al-2014-v">Inception/GoogleNet (Szegedy et al. 2014)[v]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-normalization-ioffe-and-szegedy-2015-vi">Batch Normalization (Ioffe and Szegedy 2015)[vi]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#resnet-he-et-al-2015-vii">Resnet (He et al. 2015)[vii]</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#concluding-remarks">Concluding Remarks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References:</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Development Seed
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/">
<a property="dct:title" rel="cc:attributionURL" 
 href="https://developmentseed.org/tensorflow-eo-training-2/">Deep Learning with TensorFlow and EO Data</a> was funded by <a property="dct:title" rel="cc:attributionURL"
 href="https://www.nasa.gov/servir/">NASA SERVIR</a> and is licensed under <a href="https://www.apache.org/licenses/LICENSE-2.0" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">Apache License, Version 2.0<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://www.apache.org/foundation/press/kit/asf_logo.svg"></a>
</p> 

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>